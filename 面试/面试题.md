
![112](https://gitee.com/davidq1024/blog/blob/master/images/1.jpg)

![222](https://gitee.com/davidq1024/blog/blob/master/images/1.jpg)

![333](https://image.baidu.com/search/detail?ct=503316480&z=0&ipn=d&word=%E5%9B%BE%E7%89%87&step_word=&hs=0&pn=0&spn=0&di=4730&pi=0&rn=1&tn=baiduimagedetail&is=0%2C0&istype=0&ie=utf-8&oe=utf-8&in=&cl=2&lm=-1&st=undefined&cs=1906469856%2C4113625838&os=1062705421%2C520912533&simid=3285371631%2C209838447&adpicid=0&lpn=0&ln=1396&fr=&fmq=1600534560511_R&fm=&ic=undefined&s=undefined&hd=undefined&latest=undefined&copyright=undefined&se=&sme=&tab=0&width=undefined&height=undefined&face=undefined&ist=&jit=&cg=&bdtype=0&oriquery=&objurl=http%3A%2F%2Fa2.att.hudong.com%2F36%2F48%2F19300001357258133412489354717.jpg&fromurl=ippr_z2C%24qAzdH3FAzdH3Fp7rtwg_z%26e3Bkwthj_z%26e3Bv54AzdH3Ftrw1AzdH3Fwd_nm_9b_8lnaaaa8nc0dcb8nn98d9blnc9080_3r2_z%26e3Bip4s&gsm=1&rpstart=0&rpnum=0&islist=&querylist=&force=undefined)

# 1 什么是soa？
面向服务的架构，service-oriented Architecture把系统按照实际业务，拆分成刚刚好大小的、合适的、独立部署的模块，每个模块之间相互独立。比如现我有一个数据库，一个JavaWeb（或者PHP等）的网站客户端，一个安卓app客户端，一个IOS客户端。现在我要从这个数据库中获取注册用户列表，如果不用SOA的设计思想就要写三个查询的接口，用soa可以只写一个接口，返回的数据类型是通用的json或者xml数据，就是说把这个操作封装到一个工程中去，然后暴露访问的方式，形成“服务”。

# 2 soa的好处？
1. 抽象成一个接口，改起来容易
2. 服务治理
3. 扩容


    
# 3 什么是rpc？
Remote Procedure Call   远程过程调用解决分布式系统中，服务之间的调用问题。远程调用时，要能够像本地调用一样方便，让调用者感知不到远程调用的逻辑。
  
# 4 String StringBuilder StringBuffer 有什么区别？
String不可变，StringBuilder可变，线程不安全，StringBUffer可变，线程安全
### 4.1 为什么String不可变，意义何在，为什么StringBuilder线程不安全，StringBuffer线程安全
  
# 5 为什么重写 equals 时必须重写 hashCode 方法？

# 6 mysql
### 6.1 事务四大特性？
ACID
### 6.2 并发事务带来的四个问题？
### 6.3 事务隔离级别？
1. 未提交读（会出现脏读）
2. 提交读  （不可重复读）
3. 可重复读（Innodb默认）（会出现幻读）
4. 串行化   

#### 提交读，可重复读原理
采用mvcc（多版本并发控制）来实现可重复读，select有一个读规则。可重复读的隔离下，光select不会产生幻读，有update才会幻读
解决幻读 MVCC+next-key locks

mvcc是用什么实现的：
undo log

#### 为什么项目中mysql隔离级别选择读提交
1. RR会产生间隙锁，增加死锁概率，影响效率
2. RR条件列未命中索引，会锁表，RC只锁行
3. RC可以半一致性读，update语句在加锁的行上不会等待锁

### 6.4 读写分离？
master-slave模式，master用于写数据，slave用于读数据
### 6.5 主从同步
slave的io线程从master读取二进制日志binlog，（当master更新时，master线程被激活，并将二进制日志推送给slave，slave io线程读取网络上的二进制日志binlog）
并在本地保存为中继日志relaylog，然后sql线程读取中继日志relaylog的内容并执行命令，从而保证slave和master数据同步。

### 6.6 索引原理
b+树 hash索引

# 7 分库分表
### 7.1 为什么要分库分表？
1. 数据量
2. 磁盘
3. 数据库连接

### 7.3 分库分表算法
1. 固定hash算法
2. 一致性哈希算法
3. 虚拟节点 
4. 自定义路由

### 7.4 分库分表常用策略
1. 垂直切分
2. 水平切分

### 7.2 分库分表后的问题
#### 7.2.1 分布式事务？
1. 代码里加事务 @transactional(rollbackfor = exception.class)
2. 数据库本身支持分布式事务 但是代价太大
#### 7.2.2 跨库join的问题？
1. 在代码层面实现join逻辑
#### 7.2.3分表数据量也大了之后，横向扩容的问题?
#### 7.2.4 分库分表的中间件？这几个区别？
名称| 2 | 3
--|:--:|--:
TDDL          |     属于 client 层方案 |
Sharding-jdbc  |  属于 client 层方案 |
Mycat        |        属于 proxy 层方案 
Cobar    |     属于 proxy 层方案 

#### 7.2.5 全局id生成策略
1. 自动增长列 （设置偏移量和步长，扩容的时候不好解决）
2. UUID （简单唯一，太长无序）
3. Snowflake(雪花) 算法 （64bit  41bit时间+10bit机器id+12bit流水号）

#### myisam为什么比innodb更适合读频繁，很少写的场景
1. innodb需要维护mvcc
2. innodb寻址要映射到块，再到行，MYISAM记录的直接是文件的OFFSET，定位比INNODB要快

# 8 elasticsearch
### 8.1 Elasticsearch是什么？
- elasticsearch是一个分布式的实时搜索和分析引擎

- Elasticsearch 也是 Master-slave 架构，也实现了数据的分片和备份。
- es⽂档写操作是分⽚上⽽不是节点上，先写在主分⽚，主分⽚再同步给副分⽚，因为主分⽚可以分布在不同的节点上，所以当集群只有⼀个master节点的情况下，即使流量的增加它也不会成为瓶颈，就算它挂了，任何节点都有机会成为主节点。
### 8.2 为什么使用elasticsearch
1. 存储的是非文档型数据
2. Elasticsearch使用的倒排索引比关系型数据库的B-Tree索引快
3. 文本搜索
4. 相关度排名

### 8.3什么是倒排索引
即根据文章内容中的关键字建立索引，索引会指向包含这个关键字的文档

### 8.4默认分词器
standard分词器

### 8.4 es的query大量数据报错的问题
```
SearchQuery query = new NativeSearchQueryBuilder().withQuery(matchAllQuery())
                .withTypes(TYPE_INTELLIGENT_COLLECT_LABEL.getType())
                .withIndices(EsIndex.INDEX_INTELLIGENT_BASE.getName())
                .withPageable(new PageRequest(0, 200,
                        new Sort(Sort.Direction.DESC,"createAt"))).build();

        Iterable<EntityCollectLabel> iterable = elasticsearchTemplate.queryForList(query,EntityCollectLabel.class);
```
这个方法会报数据量过大的错误
```
Caused by: QueryPhaseExecutionException[Result window is too large, from + size must be less than or equal to: [500000] but was [510000]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.]
  at org.elasticsearch.search.internal.DefaultSearchContext.preProcess(DefaultSearchContext.java:212)
```
需要用游标的方式取数据sroll api
```
String scrollId = elasticsearchTemplate.scan(searchQuery,55000,false);
 Page<EntityTrackingDi> page = elasticsearchTemplate.scroll(
                    scrollId, 55000L , new SearchResultMapper() {
```
原因 ：
***ElasticSearch的默认深度翻页机制***
ES默认的分页机制一个不足的地方是，比如有5010条数据，当你仅想取第5000到5010条数据的时候，ES也会将前5000条数据加载到内存当中，所以ES为了避免用户的过大分页请求造成ES服务所在机器内存溢出，默认对深度分页的条数进行了限制，默认的最大条数是10000条，

> 其实mysql的limit 100000,100 也是将十万多条数据加载到内存，也有同样的问题，但是mysql可以通过sql语句优化 比如
Select * From table Where ID>=(
    Select ID From table order by ID limit 100000,1
) order by ID limit 100;
SELECT id,title,content FROM items WHERE id IN (SELECT id FROM items ORDER BY id limit 900000, 10);
先查询id，这样加载到内存的只有id数据，不是全部数据。

scroll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索

es 提供了 scroll 的方式进行分页读取。原理上是对某次查询生成一个游标 scroll_id ， 后续的查询只需要根据这个游标去取数据，直到结果集中返回的 hits 字段为空，就表示遍历结束。scroll_id 的生成可以理解为建立了一个临时的历史快照，在此之后的增删改查等操作不会影响到这个快照的结果。



# 消息队列

# hashmap原理，为什么是是2的幂次方扩容

# 锁
### 锁有哪几种
ReentrantLock synchronized volitile
区别
重入  中断  公平

# 实现多线程的方式

# 观察者模式

# string,stringbuffer,stringbuilder哪些是线程安全的，哪些不安全

# string不变的好处

# 反射

# es的架构原理
分布式，分片，主分片，副分片，segment，

# mq消息队列
优点：解耦，异步，消峰
缺点：系统复杂性增加，系统的可用性降低

# rabbitmq原理
消息发送消费流程：
1. producer发送消息到mq的exchange组件
2. exchange组件根据routingkey将message分发到对应的queue1中
3. queue1将消息发送到consumer
4. consume发送ack确认消息到queue1
5. queue1收到ack，删除队列中的message
（如果queue1没收到ack，则会一直发送message，即消息丢失超时重发）

# Spring中的设计模式
1. 单例模式（bean ioc）
2. 工厂模式（bean ioc）
3. 代理模式（aop）
4. 观察者模式（applicationListener）
5.  模板方法？
6.  装饰者？

# 集合的原理
1. ArrayList
2. HashMap   (数组+链表+红黑树 线程不安全)
3. ConcurrentHashMap（线程安全）
3. HashTable   （线程安全 不为bull）
3. LinkedHashMap  （插入有序的hashmap）
4. TreeMap   （按键排序,底层是红黑树结构）
5. 

# spring事务失效的情况
1. 方法自调用
a方法没@transactional，b方法有@transactional，a调用b，代理类调用a，这时候不会触发事务。
解决方法：
 1.1 a上加@transactional，
 1.2 类上加@transactional
 1.3 a中这样调用b ((A)AopContext.currentProxy).b()
 1.4 把b方法抽到另一个类中
2. 方法不是public
3. 默认是RuntimeException，其他异常不会回滚
4. myisam引擎不支持事务

# rabbitmq的exchange路由规则
1. Fanout    （全局转发）
2. Direct   （根据routing key精确匹配）
3. Topic  （模糊匹配）


# 未解决的问题
1. synchronizd原理，偏向锁，轻量级锁，重量级锁

# JVM内存模型
1. 线程共享的：方法区，堆
2. 线程私有的，java栈，本地方法栈，程序计数器

堆：几乎所有的对象实例都在这里分配内存。
方法区：它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据
java栈：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息
本地方法栈：本地方法栈则是为虚拟机使用到的Native方法服务
程序计数器：当前线程所执行的字节码的行号指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。

# mysql选择哪一列建索引的依据
1. 列的数据发散
2. 列数据不变（是不是一点不能变）
3. 数据长度不能太长
4. 一个表不能建太多的索引
5. 联合查询的可以建联合索引

# 线程池参数设计原则
1. corePoolSize = 80%情况下每秒任务数 * 每个任务执行时间 = 200 * 0.1 = 20
2. queueCapacity = (coreSizePool/taskcost) * responsetime
\=20/0.1 * 1 = 200;
3. maxPoolSize = (max(tasks)- queueCapacity)/ * taskcost = 
(800-200)/ * 0.1 = 60
4. keepAliveTime默认

# rabbitmq消息丢失处理策略：
消息丢失分为三种，1.发送方消息发送失败，2.exhchange丢失消息，3.接收方未接受到消息
处理策略：1.发送方开启confirm模式，每个消息会带一个唯一id，exchange收到消息会返回带id的确认消息。2.exchange持久化消息（可以持久化后再返回确认消息）3.接收方接受消息后返回ack确认，自动模式（默认，一接收到就返回ack，可能没来得及处理，或者选择手动ack，处理完消息才返回ack）

exchange返回ack后，发送方会有个回调函数就行处理，知道是成功还是失败，失败的话重发消息（消息哪来的，可以存进内存，比如用hashmap），如果ack丢失，或者就是没接收到，发送方设置超时时间，超时则重发消息。

# 假如是ack在网络中丢失，发送方或者exchange重新发送消息，接收方重复消费怎么办？
在消息生产时，MQ内部针对每条生产者发送的消息生成一个inner-msg-id，作为去重和幂等的依据（消息投递失败并重传），避免重复的消息进入队列；

在消息消费时，要求消息体中必须要有一个bizId（对于同一业务全局唯一，如支付ID、订单ID、帖子ID等）作为去重和幂等的依据，避免同一条消息被重复消费。

# mq消息的顺序性

1.通过某种算法，将需要保持先后顺序的消息放到同⼀个消息队列中(kafka中就是partition,rabbitMq中就是queue)。然后只⽤⼀个消费者去消费该队列。
2.可以在消息体内添加全局有序标识来实现。

# 接口高并发的处理
1.缓存（redis）
2.限流
3.降级

### 限流
1.信号量计数器方法
```
private final Semaphore permit = new Semaphore(10, true);
@PostMapping("/test")
    public String test(){
        try {
            permit.acquire();
            log.info("处理请求===============>");
            Thread.sleep(2000);
        }catch (Exception e){
            log.error("error");
        }finally {
            permit.release();
        }
        return "success";
    }


```
2. 令牌桶算法
我们以一个恒定的速率向一个桶内放令牌，每次请求来的时候去桶里拿令牌，如果拿到了就继续后面的操作，如果没有拿到则等待。
```
public static void main(String[] args) {
        String start = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date());
        RateLimiter limiter = RateLimiter.create(1.0); // 这里的1表示每秒允许处理的量为1个
        for (int i = 1; i <= 10; i++) {
            double waitTime = limiter.acquire(i);// 请求RateLimiter, 超过permits会被阻塞
            System.out.println("cutTime=" + System.currentTimeMillis() + " call execute:" + i + " waitTime:" + waitTime);
        }
        String end = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date());
        System.out.println("start time:" + start);
        System.out.println("end time:" + end);
    }
```
RateLimiter limiter = RateLimiter.create(1.0) 创建一个限流器，每秒生成1个令牌；

limiter.acquire(i) 以阻塞的方式获取令牌，随着i的增加，需要的令牌数增多，则需要等待的时间也增加。

### 如果是秒杀，库存问题
则用redis存库存，redis库存减到0，不再处理请求

# 助力时网络故障，同一个人对同一个团助力两次
线程并发问题，两个线程同时查询助力表，发现用户a未对团b助力过，然后开始助力，这就等于a给b助力了两次，规则是一个人只能助力一次
解决办法：
1.对（a.id b.id）加唯一性索引
2.事务里代码逻辑：查询-插入-查询-校正，两个线程并发，其中一个第二次查询发现insert了两次就校正，如果两个线程都校正了，则返回助力失败。
3.insert 。。 select 语句
4.redis加分布式锁，以a的id和b的id为key加锁，锁的粒度为那一行数据。

# 更新开团表商品剩余价值时，并发执行问题
1. 可以用redis加分布式锁
2. 可以cas原理，比较version是否一样
3. 可以select  。。。。 update加行锁

mvcc只存在于读提交和 重复读 两个级别中
读提交只有行锁，默认不会行锁，需要select..update
就会行锁
重复读有next-key locks锁，是行锁和间隙锁的组合，保证不会出现幻读

# 可重复读 能防止幻读吗
能防止读情况下的幻读
mvcc+next-key防止了幻读
禁用innodb_locks_unsafe_for_binlog并且
select 。。for update就可以启用间隙锁
普通的select是快照度，肯定是防止了幻读

select ...    (快照读)
select ... for update   （当前读）
这两次结果可能不一样，不知道算不算幻读，因为不是同一种读级别

默认启用innodb_locks_unsafe_for_binlog，即有间隙锁功能

不能防止非读情况下的幻读
A事务select where id=1
    B事务insert (1,'')
A事务select where id=1
A事务insert (1,'')报错，这个算不算幻读



# sql的执行流程
1. mysql连接器做权限认证，用户名密码是否正确，该用户是否有该select语句的权限
2. 查询缓存
3. 分析器，词法分析和语法分析，词法分析就是提取关键字，如select，表名，字段名，语法分析就是sql语法是否正确
4. 优化器，以它认为的最优的执行方案去执行，比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。
5. 执行器，执行之前再校验一遍权限，调用innodb引擎的接口来执行sql

6. 如果是update的话，写undo log,用于提交失败后回滚
7. 去B+树种根据索引找到这一行数据，如果索引不是主键索引，并且查询的是不止索引字段，还要再回表一次，去主键索引的B+树查找，找到需要的那一行数据，返回
8. 判断数据页是否在内存中，是否是唯一性索引，来将update操作更改内存中的数据页或者更改磁盘文件，或者将操作缓存到changebuffer
9. 写redolog 将redo log设置为prepare状态。
10. 写binlog,redo log改成commit状态，提交事务


如果是update语句，
binlog，redolog 多两个日志模块，记录操作日志
日志文件是为了事务回滚

# mysql慢sql问题排查
1.偶尔很慢
   1.1 数据库在刷新脏页，update更新数据是先写redo log日志，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。如果redolog日志满了，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，就会导致我们平时正常的SQL语句突然执行的很慢
   1.2 拿不到锁，如果要判断是否真的在等待锁，我们可以用 show processlist这个命令来查看当前的状态
   
2. 一直很慢
    2.1 where字段没有索引
    2.2 没用上索引，例如where c-1=100,where pow(c,2) = 1000,
    2.3 数据库选错索引，例如，select * from t where 100 < c and c < 100000，走索引的话需要走两次，一次c索引，一次主键索引，可能不如扫描，只要扫描一次。所以mysql怎么判断走不走索引，采样部分数据，如果数据发散厉害，范围大，就走索引，如果发散范围不大，就不走索引，但是采样可能出现错误，刚好采样到范围小的数据，就导致选错索引。解决办法：强制使用索引，select * from t force index(a) where c < 100 and c < 100000;  采样错误也会导致where多个字段，有索引的话，选错索引
    
登录到MySQL中进行查看是否有事物未提交，是否有发生锁等待。
select * from information_schema.INNODB_TRX
select * from information_schema.PROCESSLIST #查看当前的SQL执行情况。主要观察是否有 Waiting for table metadata lock 或者表锁、全局读锁等SQL。

看看是否有大量临时表


# undo log是什么？
undo log主要是保证事务的原子性，事务执行失败就回滚，用于在事务执行失败后，对数据回滚。undo log是逻辑日志，记录的是SQL。（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。）

# change buffer是什么
（就是将更新数据页的操作缓存下来）
在更新数据时，如果数据行所在的数据页在内存中，直接更新内存中的数据页。
如果不在内存中，为了减少磁盘IO的次数，innodb会将这些更新操作缓存在change buffer中，在下一次查询时需要访问这个数据页时，在执行change buffer中的操作对数据页进行更新。
适合写多读少的场景，因为这样即便立即写了，也不太可能会被访问到，延迟更新可以减少磁盘I/O，只有普通索引会用到，因为唯一性索引，在更新时就需要判断唯一性，所以没有必要。

# redo log 是什么？
redo log就是为了保证事务的持久性。因为change buffer是存在内存中的，万一机器重启，change buffer中的更改没有来得及更新到磁盘，就需要根据redo log来找回这些更新。
优点是减少磁盘I/O次数，即便发生故障也可以根据redo log来将数据恢复到最新状态。
缺点是会造成内存脏页，后台线程会自动对脏页刷盘，或者是淘汰数据页时刷盘，此时收到的查询请求需要等待，影响查询。

# cas的缺点
1. 循环时间长开销大
2. 只能保证一个共享变量的原子操作
3. ABA问题

# java中cas的应用
1. AtomicInteger
2. 自旋锁
3. 令牌桶限流器

# 缓解数据库压力的办法
1. 缓存
2. 分库分表
3. 读写分离
4. 合理增加索引

# 什么是redis
2.Reids的特点
Redis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。

Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能，比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。

Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。

# redis应用场景
分布式缓存  
分布式锁    数据库并发插入，秒杀系统
计数器     网站浏览量，视频播放次数
轻量的消息队列
排行榜 zset
最新列表  list
存储session   博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。

# redis数据类型
Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。

String:
会采用预分配冗余空间的方式来减少内存的频繁分配，实际分配的空间 capacity 一般要高于实际字符串长度 len 
当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。
list：
list 采用的存储结构是双向链表
在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成 quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next。所以 Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。
Redis hash 是一个键值(key=>value)对集合。
hash 与 Java 中的 HashMap 差不多，实现上采用二维结构，第一维是数组，第二维是链表。hash 的 key 与 value 都存储在链表中，而数组中存储的则是各个链表的表头。在检索时，首先计算 key 的 hashcode，然后通过 hashcode 定位到链表的表头，再遍历链表得到 value 值。
初始化大小为4


set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。



# redis hash
在redis的具体实现中，使用了一种叫做渐进式哈希(rehashing)的机制来提高字典的缩放效率，避免 rehash 对服务器性能造成影响，渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。

扩容是扩容为原来的2倍，如果原来不是2的n次方
则是大于等于size的第一个2的n次方

size初始化为4
hash大小size同时也是扩容的阈值

dictht ht[2]：在字典内部，维护了两张哈希表。 一般情况下， 字典只使用 ht[0] 哈希表， ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用
```
typedef struct dict {
    dictType *type;
    void *privdata;
    dictht ht[2];
    long rehashidx; /* rehashing not in progress if rehashidx == -1 */
    unsigned long iterators; /* number of iterators currently running */
} dict;

/* This is our hash table structure. Every dictionary has two of this as we
 * implement incremental rehashing, for the old to the new table. */
typedef struct dictht {
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigned long used;
} dictht;

typedef struct dictEntry {
    void *key;                //键
    union {
        void *val;            //值
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    struct dictEntry *next; //指向下一个节点，形成链表
} dictEntry;
```
扩容条件 
```
/*
     * 如果哈希表ht[0]中保存的key个数与哈希表大小的比例已经达到1:1，即保存的节点数已经大于哈希表大小
     * 且redis服务当前允许执行rehash，或者保存的节点数与哈希表大小的比例超过了安全阈值（默认值为5）
     * 则将哈希表大小扩容为原来的两倍
     */
 if (d->ht[0].used >= d->ht[0].size &&
        (dict_can_resize ||
         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio))
    {
        return dictExpand(d, d->ht[0].used*2);
    }
```

缩容
当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。
缩容后的大小为第一个大于等于当前key数量的2的n次方。最小容量为4。
同样从dictResize函数中可以看到，如果当前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，则不进行缩容

## 渐进式hash小结
在redis中，扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面， 但是， 这个 rehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。

以下是哈希表渐进式 rehash 的详细步骤：

（1）为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。

（2）在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。

（3）在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。

（4）随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。

渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。

# rehash的其他细节和缺点
渐进式 rehash 执行期间的哈希表操作
因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。

另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。

#渐进式rehash带来的问题
渐进式rehash避免了redis阻塞，可以说非常完美，但是由于在rehash时，需要分配一个新的hash表，在rehash期间，同时有两个hash表在使用，会使得redis内存使用量瞬间突增，在Redis 满容状态下由于Rehash会导致大量Key驱逐。


# redis主从同步

# 单线程的redis为什么这么快
(一)纯内存操作
(二)单线程操作，避免了频繁的上下文切换
(三)采用了非阻塞I/O多路复用机制

# redis的四个问题
(一)缓存和数据库双写一致性问题
    正常流程：先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。
(二)缓存雪崩问题
(三)缓存穿透问题
(四)缓存的并发竞争问题

# redis删除策略
redis采用的是定期删除+惰性删除策略。
定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。

于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。

采用定期删除+惰性删除就没其他问题了么?
不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。

# redis淘汰策略
noeviction(默认策略)：对于写请求不再提供服务，直接返回错误（DEL请求和部分特殊请求除外）
volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
allkeys-lru：（推荐）从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰
allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰
no-enviction（驱逐）：禁止驱逐数据

# 缓存穿透
缓存穿透是指查询一个一定不存在的数据（数据库也不存在），由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。
***解决方案：***
(一)利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试
(二)采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。
（三）布隆过滤器


有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。

布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。

# 缓存雪崩
缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。
***解决方案：***
这里分享一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。

# 缓存并发竞争问题
(1)如果对这个key操作，不要求顺序
这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。
(2)如果对这个key操作，要求顺序
假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.
期望按照key1的value值按照 valueA-->valueB-->valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下

系统A key 1 {valueA  3:00}
系统B key 1 {valueB  3:05}
系统C key 1 {valueC  3:10}
那么，假设这会系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。

其他方法，比如利用队列，将set方法变成串行访问也可以。总之，灵活变通。

# redis分布式锁是怎么实现的
使用setnx方法来争抢锁，抢到之后，再用expire方法给锁加一个过期时间防止锁忘记了释放。 如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？
set指令参数，可以同时把setnx和expire合成一条指令来用
使用 lua 脚本
```
if (setnx(key, 1) == 1){
    expire(key, 30)
    try {
        //TODO 业务逻辑
    } finally {
        del(key)
    }
}
```

## 锁误解除
如果线程 A 成功获取到了锁，并且设置了过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁；随后 A 执行完成，线程 A 使用 DEL 命令来释放锁，但此时线程 B 加的锁还没有执行完成，线程 A 实际释放的线程 B 加的锁。

解决办法：通过在 value 中设置当前线程加锁的标识，在删除之前验证 key 对应的 value 判断锁是否是当前线程持有。可生成一个 UUID 标识当前线程，使用 lua 脚本做验证标识和解锁操作。

## 超时解锁导致并发
如果线程 A 成功获取锁并设置过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁，线程 A 和线程 B 并发执行。

解决办法：
1.将过期时间设置足够长，确保代码逻辑在锁释放之前能够执行完成。
2.为获取锁的线程增加守护线程，为将要过期但未释放的锁增加有效时间。

# redis主从复制
为了保证 Redis 的可用性，一般采用主从方式部署。主从数据同步有异步和同步两种方式，Redis 将指令记录在本地内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一致的状态，一边向主节点反馈同步情况。

# redis缓存数据库双写一致性
有四种方案
1.更新数据库，更新缓存
2.更新缓存，更新数据库
3.删除缓存，更新数据库
4.更新数据库，删除缓存

1. 1的问题在于，如果缓存不是直接拿的数据库值，而是经过一个复杂的计算，则这样每次都重新计算下，但是又不怎么读这个数，太费资源。
2. 2的问题和1的问题一样
3. 3的问题在于，并发请求时，请求a删除redis，更新数据库，但没提交，这时b查询，redis没有数据，数据库因为还没提交也没数据，解决方法：事务包括删redis，更新数据库，再删除redis？
4. 正确的做法，但是还是有问题：更新数据库后，删除缓存失败。解决：删除缓存失败后将key值扔进消息队列，再从消息队列中取key来删除，问题：和业务代码耦合太深，解决:从binlog日志中读取对mysql的操作，然后将key扔进mq中再删除

# new一个对象发生了什么
（下面的是简版）
1. 加载。加载类信息到方法区
2. 验证。验证格式，语义，(文件规范，final有没有子类）
3. 准备。为静态变量分配内存空间
4. 解析
5. 初始。为静态变量赋值，执行static代码块
6. 创建对象。堆区为对象分配内存。执行初始化代码

# java8对synchronized的优化
java6引进四种锁的状态，无锁，偏向锁，轻量级锁，重量级锁，（偏向锁和轻量级锁应该用的cas原理）
java8对cas进行优化，同时大量线程cas，会导致到家都不能成功，使用分段cas的思路，内部维护一个cell数组，分别对数组的一个位置操作，最后将结果累加。比如cell大小为10，100个线程，则10个线程对cell[0]自增，10个线程对cell[1]自增。。。 

# reentrantlock和synchronized的区别
1.实现方式，synchronized是java关键字，jvm层面实现锁，reentrantlock是api
2.等待可中断。reentrantlock等待长时间未果，可以中断去执行其他事情
3.公平锁。reentrantlock可以实现公平锁，synchronized是非公平锁
4.锁绑定多个条件

# JUC包有哪些类
java.util.concurrent 即java的并发包
1. ReentrantLock
2. AtomicInteger
3. CountDownLatch
4. ConcurrentHashmap
5. ThreadPoolExecuter
6. .....

# aqs解释
https://zhuanlan.zhihu.com/p/86072774

# synchronized同步原理
https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&mid=2651749434&idx=3&sn=5ffa63ad47fe166f2f1a9f604ed10091&chksm=bd12a5778a652c61509d9e718ab086ff27ad8768586ea9b38c3dcf9e017a8e49bcae3df9bcc8&scene=38#wechat_redirect

Monitor可以理解为一个同步工具或一种同步机制，通常被描述为一个对象。每一个Java对象就有一把看不见的锁，称为内部锁或者Monitor锁。

Monitor是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联，同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。

现在话题回到synchronized，synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的线程同步。

如同我们在自旋锁中提到的“阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长”。这种方式就是synchronized最初实现同步的方式，这就是JDK 6之前synchronized效率低的原因。这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”，JDK 6中为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”。


# java解决幻读的问题
解决方式是在select读时候的sql中增加for update  , 会把我所查到的数据锁住 , 别的事务根本插不进去 , 这样就解决了,这里用到的是mysql的next-key locks

SELECT ... FOR UPDATE 走的是IX锁(意向排它锁)，即在符合条件的rows上都加了排它锁，其他session也就无法在这些记录上添加任何的S锁或X锁。如果不存在一致性非锁定读的话，那么其他session是无法读取和修改这些记录的，但是innodb有非锁定读(快照读并不需要加锁)，for update之后并不会阻塞其他session的快照读取操作，除了select ...lock in share mode和select ... for update这种显示加锁的查询操作。

# spring事务失效的场景
1. 底层数据库引擎不支持事务
2. 方法非public
3. 事务方法里使用了try-catch
4. 一个类的A方法（未加事务）内部调用了自己的B方法（加了事务）
5. rollbackfor设置错误
6. propagation设置错误 supports not_supported never


# 线程同步的方法
1. synchronized
2. reentrantlock
3. linkedBlockingQueue
4. AtomicInteger

# redis高并发和快速的原因
1.redis是基于内存的，内存的读写速度非常快；

2.redis是单线程的，省去了很多上下文切换线程的时间；不需要各种锁的性能消耗。 

3.redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。

# redis为什么是单线程的
因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。

# epoll原理

# java接口架构图
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_1.png)

4c0ea9d4d39c4ab09ed7e81ac76993d1.jpg

# fork join


# JVM知识点
在书上笔记

# mybatis原理
mybatis访问数据库有两种方式
1. mybatis api
创建一个和数据库打交道的SqlSession对象，然后根据Statement Id 和参数来操作数据库
2. mapper接口
通过动态代理机制生成一个Mapper 实例，我们使用Mapper接口的某一个方法时，MyBatis会根据这个方法的方法名和参数类型，确定Statement Id，底层还是通过SqlSession.select("statementId",parameterObject);或者SqlSession.update("statementId",parameterObject); 等等来实现对数据库的操作

原理
MyBatis启动时，解析mybatis的配置文件，并且从指定路径下解析mapper.xml配置文件 
把每条sql语句映射成MappedStatement
然后把MappedStatement存放到Configuration的一个mappedStatements属性中（mappedStatements是一个HashMap），key为namespace + id，value为MappedStatement
当要执行sql语句的时候，从mappedStatements这个map中通过id找到MappedStatement
获取MappedStatement对应sql语句、查询参数
查看一级缓存中有没有数据，有则直接返回
缓存没有数据，则查询数据库 
通过调用原生的jdbc方法，执行sql语句，获取到结果，删除旧缓存
把结果放到一级缓存，返回结果

# mybatis架构图
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_2.png)

https://blog.csdn.net/luanlouis/article/details/40422941

https://www.jianshu.com/p/ec40a82cae28


# 接口反应时间长的问题排查（接口慢）
1. 资源瓶颈 （加机器）
2. 线程池配置的不合理（线程数配置的太少导致的请求积压）
3. 是否多表关联  把join逻辑放到业务代码里解决
2. 缓存没加 （加缓存）
3. 查看gc是否频繁GC，GC时间过长
4. 依赖第三方接口 （是不是第三方接口反应慢，能不能异步调用）
5. sql查询慢 （大量临时表，索引有没有加，explain有没有用到索引，是否需要强制需要用索引，数据量太大，要扩容）

# 聚簇索引和非聚簇索引区别
1. 聚簇索引的主键索引叶节点存放整行数据，非聚簇索引叶节点存放数据地址
2. 聚簇索引的数据的物理存放顺序与索引顺序是一致的

# 聚簇索引的好处
1. InnoDB在移动行时无须更新辅助索引中的这个"指针"
2. 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问，不必访问磁盘。
3. 聚簇索引适合用在排序的场合，非聚簇索引不适合
4. 取出一定范围数据的时候，使用用聚簇索引
5. 非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转。聚簇索引则只需一次I/O。

# 聚簇索引劣势
1. 二级索引访问需要两次索引查找
2. 插入速度严重依赖于插入顺序
3. 更新主键的代价很高，因为将会导致被更新的行移动
4. 采用聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多（https://juejin.im/post/6844903845554814983）

# ConcurrentHashmap原理
1. 线程安全的，1.7之前是分段锁，1.8是cas+synchronized
put操作分两种情况，
1. 当前hash表对应当前key的index上没有元素时
cas的方式创建node放入数组
2.当前hash表对应当前key的index上已经存在元素时(hash碰撞)
synchronized对当前位置的节点头 加锁
> https://juejin.im/post/6844903813892014087

# rabbitmq kafka rocketmq区别
1. kafka：吞吐量高，消息失败不支持重试,轻量级的，高效的但是不保证安全的。一开始的目的就是用于日志收集和传输，适合产生大量数据的互联网服务的数据收集业务。
2. rabbitmq:对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。有消息确认机制，和持久化功能。
开源的，社区比较稳定的支持，活跃度也高，社区文档资料完善些
3. RocketMQ 阿里开源的消息中间件，它是纯Java开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。

选型：
所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。
如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。

# spring bean创建过程


# spring如何解决循环依赖
首先，需要明确的是spring对循环依赖的处理有三种情况：
①构造器的循环依赖：这种依赖spring是处理不了的，直 接抛出BeanCurrentlylnCreationException异常。
②单例模式下的setter循环依赖：通过“三级缓存”处理循环依赖。
③非单例循环依赖：无法处理。

2的情况原理：
spring三级缓存
我们初始化一个Bean时，先调用Bean的构造方法，这个对象就在内存中存在了（对象里面的依赖还没有被注入），然后把这个对象保存下来，当循环依赖产生时，直接拿到之前保存的对象，于是循环依赖就被终止了，依赖注入也就顺利完成了。

https://blog.csdn.net/lkforce/article/details/97183065
https://juejin.im/post/6844903806757502984

# java怎么解决循环依赖
1. 重新设计类结构
2. 注解 @Lazy
```
@Component
public class CircularDependencyA {
 
    private CircularDependencyB circB;
 
    @Autowired
    public CircularDependencyA(@Lazy CircularDependencyB circB) {
        this.circB = circB;
    }
}
```
3. 使用Setter/Field注入
```
@Autowired
    public void setCircB(CircularDependencyB circB) {
        this.circB = circB;
    }
```
4. 使用@PostConstruct
```
@Component
public class CircularDependencyA {
 
    @Autowired
    private CircularDependencyB circB;
 
    @PostConstruct
    public void init() {
        circB.setCircA(this);
    }
 
    public CircularDependencyB getCircB() {
        return circB;
    }
}
```
5. 实现ApplicationContextAware与InitializingBean

https://blog.csdn.net/Revivedsun/article/details/84642316


# sql：while col1 like '%key' 怎么用索引
//创建一个函数索引
CREATE INDEX inde_1 ON table1(REVERSE(col1));
SELECT * FROM table1 WHERE REVERSE(col1) LIKE REVERSE('%ABC');

# @autowired原理
根据类型自动装配
@Autowired   
@Qualifier("userServiceImpl")   
public IUserService userService; 
加上@Qualifier 可以根据名字就行装配

# explain除了看索引还看什么 
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_3.png)

# mybatis二级缓存

# redis zset原理

# g1垃圾收集器，强软弱虚引用，gc算法

# threadlocal
定义： 可以创建线程私有变量
作用：    1.实现线程安全
        2. 实现单个线程单例以及单个线程上下文信息存储，比如交易id等
        3. 承载一些线程相关的数据，避免在方法中来回传递参数
原理
threadlocal.set()
就是往threadlocalMap里set东西
ThreadLocalMap是ThreadLocal的内部类
map中key是当前ThreadLocal对象
Thread为每个线程维护了ThreadLocalMap这么一个Map，而ThreadLocalMap的key是LocalThread对象本身，value则是要存储的对象
每个Thread维护着一个ThreadLocalMap的引用


# ThreadLocal内存泄漏
原因：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏

可是ThreadLocal并不会产生内存泄露，因为ThreadLocalMap在选择key的时候，并不是直接选择ThreadLocal实例，而是ThreadLocal实例的弱引用。

map里是弱引用，只要gc就会回收，不会发生内存泄漏



# volitile防止指令重排原理
编译后生成 内存屏障指令
内存屏障指令前面的代码不可以越过这个屏障，后面的代码也不能越过这个屏障

# cpu100%排查 ? 待完善
1. 定位高负载进程 pid，使用top命令
2. 定位具体的异常业务，使用 pwdx 命令根据 pid 找到业务进程路径，进而定位到负责人和项目
3. 定位异常线程及具体代码行
传统的方案一般是4步：

top oder by with P：1040 // 首先按进程负载排序找到  maxLoad(pid)
top -Hp 进程PID：1073    // 找到相关负载 线程PID
printf “0x%x\n”线程PID： 0x431  // 将线程PID转换为 16进制，为后面查找 jstack 日志做准备
jstack  进程PID | vim +/十六进制线程PID -        // 例如：jstack 1040|vim +/0x431 -


1. 定位高负载进程 pid，使用top命令
2. 显示线程列表 ps -mp pid -o THREAD,tid,time

# 淘宝首页性能优化
从技术的角度

# redis缓存 雪崩 击穿 穿透
雪崩：
本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，或者那一批缓存全部同时到期，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。
同时到期：则在时间上加上一个随机值
意外宕机解决方案：
缓存雪崩的事前事中事后的解决方案如下。
事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。
事中：本地 ehcache 缓存 + hystrix 限流&降级，避免 MySQL 被打死。
事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。

缓存穿透：
大量的请求，缓存里查不到，数据库也查不到，但是请求全部落在数据库上，恶意攻击
解决:1.每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去. 2.布隆过滤器

缓存击穿：
缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库。
解决： 
1. 将热点数据设置为永远不过期
2. 或者基于 redis or zookeeper 实现互斥锁，缓存失效后，等待失效后的第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。

# InnoDB事务实现原理
acid
原子性：
靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。
undo log属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。
以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。

一致性：

隔离性:
(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性

持久性：
靠的是redo log。当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。


# innodb和myisam的区别
1. innodb不保存表的具体行数
为什么innodb不保存：
因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询
2. InnoDB 支持事务，MyISAM 不支持事务
3. InnoDB 支持外键，而 MyISAM 不支持
4. InnoDB 是聚集索引，MyISAM 是非聚集索引。
5. InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁

# 内存泄漏怎么排查问题

# 使用反射 怎么实现一个rpc框架

# mybatis怎么写 like查询，即不同于普通的like查询

# mybatis二级缓存

# redis zet原理

# redis需要掌握的
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_4.png)

# explain怎么用
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_5.png)

主要看哪些字段：
type：
一般来说，得保证查询至少达到range级别，最好能达到ref。
key:
列显示MySQL实际决定使用的键（索引）
key_len：
显示MySQL决定使用的键长度
Extra：
Using temporary 用临时表保存中间结果，常用于GROUP BY 和 ORDER BY操作中，一般看到它说明查询需要优化了，就算避免不了临时表的使用也要尽量避免硬盘临时表的使用。


# 线程安全的hashmap
hashtable  set get方法上加synchronized
synchronizedMap    synchronized获取mutex锁
concurrentHashMap


# 线程池如何保证核心线程数量不变 ？待完善
线程池当未调用 shutdown 方法时，是通过队列的 take 方法阻塞核心线程（Worker）的 run 方法从而保证核心线程不被销毁的。

核心线程数未超过 corePoolSize，每添加新的任务（command），都会创建新的线程（Worker中创建），即使有空闲线程存在；
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_6.png)

https://blog.csdn.net/smile_from_2015/article/details/105259789


# es深度分页解决方案  scrollid会变吗
https://my.oschina.net/u/1787735/blog/3024051

# es调优，慢查询怎么处理

# @autowired  类型注入还是名字注入


线程池怎么保证core线程数量不变
g1的原理，好处
分库分表是对 groupid这个字段进行hash，这有什么缺陷
synchronized无锁状态怎么实现的
es怎么提高查询效率
spring原理，动态代理原理，rpc框架动态代理原理
lru
threadlocal用法

# B树相对B+树的优点

# spring bean初始化流程

# spring ioc启动过程

# jvm gc算法 g1的特点

# jvm类加载机制

# threadlocal 弱引用 内存泄漏原理

# mysql事务执行过程

# cpu利用率高怎么排查

# spring循环依赖

# bean工厂和工厂bean

# mysql分库分表 平滑扩容方案

# 接口响应时间长调优

