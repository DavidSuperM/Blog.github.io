
# 1 什么是soa？
面向服务的架构，service-oriented Architecture把系统按照实际业务，拆分成刚刚好大小的、合适的、独立部署的模块，每个模块之间相互独立。比如现我有一个数据库，一个JavaWeb（或者PHP等）的网站客户端，一个安卓app客户端，一个IOS客户端。现在我要从这个数据库中获取注册用户列表，如果不用SOA的设计思想就要写三个查询的接口，用soa可以只写一个接口，返回的数据类型是通用的json或者xml数据，就是说把这个操作封装到一个工程中去，然后暴露访问的方式，形成“服务”。

# 2 soa的好处？
1. 抽象成一个接口，改起来容易
2. 服务治理
3. 扩容
    
# 3 什么是rpc？
Remote Procedure Call   远程过程调用解决分布式系统中，服务之间的调用问题。远程调用时，要能够像本地调用一样方便，让调用者感知不到远程调用的逻辑。
  
# 4 String StringBuilder StringBuffer 有什么区别？
String不可变，StringBuilder可变，线程不安全，StringBuffer可变，线程安全

### 4.1 为什么String不可变
<https://www.cnblogs.com/wkfvawl/p/11693260.html>

### 4.2 为什么StringBuilder可变，线程不安全，StringBuffer可变，线程安全
见博客
<https://juejin.cn/post/6844903923908788237>
<https://blog.csdn.net/codingisforlife/article/details/100166513>

### 4.3 既然StringBuilder线程不安全，为什么一般还是用StringBuilder
方法内的局部变量，不存在线程安全的问题，肯定StringBuilder。
什么是线程安全。核心点其实很简单，多线程必须访问同一个对象才有可能产生线程安全的问题。
在一个方法内部，声明了一个对象，只要你没有在这个方法里面在创建线程使用这个对象，人为构建多线程的环境，那么这都不是多线程，进而也没有线程安全问题了。
但如果类似于类的实例变量（在controller里面定义一个全局变量）这样，由于spring MVC的机制（controller是单例，tomcat容器接收请求后是多线程响应），每个controller的方法都可能会被多个线程调用，这个时候，类的实例变量就存在线程安全问题了，所以应该用线程安全的类。

  
# 5 为什么重写equals方法时必须重写hashCode方法？
equals方法是超类Object中的一个基本方法，用于检测一个对象是否与另外一个对象相等。默认是比较引用值。
hashCode方法的意思就是散列码，也就是哈希码，返回的是将对象的地址值映射为integer类型的哈希值
hashCode方法中有约定：“相同对象必须有相同哈希值” (但是有相同hashCode值的对象不一定相等)
为什么有这个约定以及需要重写主要因为set，map集合的原理。
Set集合中元素是无序不可重复的。因此插入数据时会先比较两个元素hashCode值是否一致，不一致则集合中没有重复元素，插入。hashCode一致的话再调用equals方法比较两个元素是否一致。
（为什么不直接比较equals方法呢，因为只用equals方法的话，新插入一个元素需要和set集合内所有元素比较，费时。先获取hashcode值再取余的方法得到元素应该存储在set集合中的位置，再和指定位置上的元素比较equals，效率高）
因此假如只重写User类的equals方法，重写为比较User对象的所有属性值是否相等。不重写User类的hashCode方法。
set集合中已有User(name="david",age=18)的对象user1，再new一个属性相同的对象后，因为hashCode值和user1不同，计算在set集合数组中位置也不同，最后成功插入到set集合中。违法了set集合中不能有相同对象的约定。

参考<https://juejin.cn/post/6844903854639693837>
<https://blog.csdn.net/u013679744/article/details/57074669>

### 5.1 重写equals需要遵守的四大特性
参考<https://blog.csdn.net/javazejian/article/details/51348320>

# 6 mysql

### 6.1 数据库事务四大特性？
ACID
6.1.1 原子性（Atomicity）
原子性是指事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败。
6.1.2 一致性（Consistency）
事务必须使数据库从一个一致性状态变换到另外一个一致性状态。
> 举例说明：张三向李四转100元，转账前和转账后的数据是正确的状态，这就叫一致性，如果出现张三转出100元，李四账号没有增加100元这就出现了数据错误，就没有达到一致性。

6.1.3 隔离性（Isolation）
事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。
6.1.4 持久性（Durability）
持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。

### 6.2 并发事务带来的四个问题？

### 6.3 数据库事务隔离级别（泛指所有数据库，不是特指mysql）
1. 未提交读（RU）（会出现脏读）
2. 提交读（RC）  （不可重复读）
3. 可重复读（RR）（Innodb默认）（会出现幻读）
4. 串行化 
详细介绍：<https://github.com/Snailclimb/JavaGuide/blob/main/docs/database/mysql/transaction-isolation-level.md>

### 6.4 Mysql MVCC
mysql中提交读，可重复读 是通过MVCC（多版本并发控制）来实现的。
mvcc只在提交读和可重复读的隔离级别下才起作用。
mvcc原理分析：<https://juejin.cn/post/6844904194206351373>
> 但是这个博客在讲 REPEATABLE READ 下第一张图里 min_trx_id 值写错了，应该是101。

### 6.5 为什么项目中mysql隔离级别选择提交读
1. RR会产生间隙锁，增加死锁概率，影响效率
2. RR条件列未命中索引，会锁表，RC只锁行
3. RC可以半一致性读，update语句在加锁的行上不会等待锁
参考<https://zhuanlan.zhihu.com/p/59061106>

### 6.6 共享锁，排它锁
共享锁和排它锁 和数据库隔离级别无关，任何隔离级别都可以有这两个锁

共享锁又称为读锁，简称S锁。多个事务的查询语句可以共用一把共享锁；
例如事务1在id=1的记录加上共享锁，事务2可以读id=1的记录，但不能修改删除
使用共享锁
```
SELECT * FROM `test` WHERE `id` = 1 LOCK IN SHARE MODE;
```
多个事务的查询语句可以共用一把共享锁；
如果只有一个事务拿到了共享锁，则该事务可以对数据进行 UPDATE DELETE 等操作；
如果有多个事务拿到了共享锁，则所有事务都不能对数据进行 UPDATE DELETE 等操作。

排他锁又称为写锁，简称X锁，顾名思义，排它锁不能与其它锁并存，而且只有一个事务能拿到某一数据行的排它锁，其余事务不能再获取该数据行的所有锁。
使用排它锁
```
SELECT * FROM `test` WHERE `id` = 1 FOR UPDATE;
```

### 6.7 什么是间隙锁
间隙锁说明：
当我们用范围条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁。对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。
例如
```
现有数据 
id age
1  10
2  20
3  40
4  50

SELECT * FROM child WHERE age >= 20 and age<=30 FOR UPDATE;
```
间隙锁会将age20-40范围内数据锁住（间隙锁会产生在现有数据之间，而不是20-30的间隙），其他事物不能在这个区间插入或修改数据

间隙锁的启用：mysql隔离级别设置为RR，并且关闭参数innodb_locks_unsafe_for_binlog （show variables like 'innodb_locks_unsafe_for_binlog'; 查看是否启用间隙锁）

MySQL InnoDB支持三种行锁定方式：
行锁（Record Lock）：锁直接加在索引记录上面。
间隙锁（Gap Lock）：锁加在不存在的空闲空间，可以是两个索引记录之间，也可能是第一个索引记录之前或最后一个索引之后的空间。
临键锁（Next-Key Lock）：行锁与间隙锁组合起来用就叫做Next-Key Lock。

间隙锁只存在于RR和串行话隔离级别中，RC和RU隔离级别只会产生行锁。

where条件列是唯一索引时，只有锁住多条记录或者一条不存在的记录的时候，才会产生间隙锁，指定给某条存在的记录加锁的时候，只会加记录锁，不会产生间隙锁。
where条件列是普通索引时，不论条件是单条还是范围，都会产生间隙锁。
where条件列没有索引时，不论条件是单条还是范围，都会产生间隙锁。和普通索引的区别在于会先对全表加锁，然后再释放不满足条件的记录，对满足条件的范围记录加锁，所以在没有索引时，不满足条件的数据行会有加锁又放锁的耗时过程。

参考：<https://www.cnblogs.com/rjzheng/p/9950951.html>

### 6.8 mysql RR隔离级别下会产生幻读吗
结论：因为MVCC机制，可以防止读情况下的幻读，但是不能防止其他情况的幻读（例如update，delete）。但是一开始如果加上间隙锁，则可以防止所有情况的幻读。

详细说明：
|  id   | name  |
|  ----  | ----  |
| 1  | 后勤部 |

|  事务1                                                      | 事务2  |
|  ----                                                      | ----  |
| begin                                                      | begin |
| select * from dept                                         |       |
|                                                            |  insert into dept(name) values("研发部")  |
|                                                             |  commit |
| select * from dept (查出来一条记录，没有出现幻读，因为mvcc机制，新插入的数据版本高，不可见)  |    |
| update dept set name="财务部"   （会出现幻读，即更新了两条数据）    |   |
| commit                                                        |   |

结果
|  id   | name  |
|  ----  | ----  |
| 1  | 财务部 |
| 2  | 财务部 |

结论：第二次select没有出现幻读，update出现了幻读的情况，MySQL可重复读的隔离级别中mvcc机制并不是完全解决了幻读的问题，而是解决了读数据情况下的幻读问题。update会在最新的记录上操作。
但是如果第一次select 后面加上 for update 间隙锁，那么就不会出现幻读的情况


### java解决幻读的问题
解决方式是在select读时候的sql中增加for update  , 会把我所查到的数据锁住 , 别的事务根本插不进去 , 这样就解决了,这里用到的是mysql的next-key locks

SELECT ... FOR UPDATE 走的是IX锁(意向排它锁)，即在符合条件的rows上都加了排它锁，其他session也就无法在这些记录上添加任何的S锁或X锁。如果不存在一致性非锁定读的话，那么其他session是无法读取和修改这些记录的，但是innodb有非锁定读(快照读并不需要加锁)，for update之后并不会阻塞其他session的快照读取操作，除了select ...lock in share mode和select ... for update这种显示加锁的查询操作。

# select加锁分析
<https://www.cnblogs.com/rjzheng/p/9950951.html>


### 6.9 Redo log, bin log, Undo log
InnoDB中通过undo log实现了数据的多版本，而并发控制通过锁来实现。  
undo log除了实现MVCC外，还用于事务的回滚。MySQL Innodb中存在多种日志，除了错误日志、查询日志外，还有很多和数据持久性、一致性有关的日志。  
binlog，是mysql服务层产生的日志，常用来进行数据恢复、数据库复制，常见的mysql主从架构，就是采用slave同步master的binlog实现的, 另外通过解析binlog能够实现mysql到其他数据源（如ElasticSearch)的数据复制。  
redo log记录了数据操作在物理层面的修改，mysql中使用了大量缓存，缓存存在于内存中，修改操作时会直接修改内存，而不是立刻修改磁盘，当内存和磁盘的数据不一致时，称内存中的数据为脏页(dirty page)。为了保证数据的安全性，事务进行中时会不断的产生redo log，在事务提交时进行一次flush操作，保存到磁盘中, redo log是按照顺序写入的，磁盘的顺序读写的速度远大于随机读写。当数据库或主机失效重启时，会根据redo log进行数据的恢复，如果redo log中有事务提交，则进行事务提交修改数据。这样实现了事务的原子性、一致性和持久性。  
undo log: 除了记录redo log外，当进行数据修改时还会记录undo log，undo log用于数据的撤回操作，它记录了修改的反向操作，比如，插入对应删除，修改对应修改为原来的数据，通过undo log可以实现事务回滚，并且可以根据undo log回溯到某个特定的版本的数据，实现MVCC。  

### 6.10 读写分离？
master-slave模式，master用于写数据，slave用于读数据
### 6.11 主从同步
slave的io线程从master读取二进制日志binlog，（当master更新时，master线程被激活，并将二进制日志推送给slave，slave io线程读取网络上的二进制日志binlog）
并在本地保存为中继日志relaylog，然后sql线程读取中继日志relaylog的内容并执行命令，从而保证slave和master数据同步。

### 6.12 索引原理
Mysql myisam b+树 非聚簇索引
Mysql innodb b+树 聚簇索引

b树 b+树 hash索引
hash索引的缺点：Hash 索引不支持顺序和范围查询 例如
```
SELECT * FROM tb1 WHERE id < 500
```
b+树直接遍历比500小的叶子节点即可，hash索引要把id 1-499 的都要hash计算位置，太费时

b树的缺点：
- B 树的所有节点既存放键(key) 也存放 数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。（因此b+树一页可以存放更多的索引，树高度就会降低）
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。（因此b+适用于范围查询，效率高）

参考<https://github.com/Snailclimb/JavaGuide/blob/main/docs/database/mysql/mysql-index.md>

### 7 分库分表
见<https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/DB/mysql%E5%88%86%E5%8C%BA%E5%88%86%E8%A1%A8%E5%88%86%E5%BA%93%E6%96%B9%E6%A1%88.md>


#### myisam为什么比innodb更适合读频繁，很少写的场景
1. innodb需要维护mvcc
2. innodb寻址要映射到块，再到行，MYISAM记录的直接是文件的OFFSET，定位比INNODB要快

### mysql选择哪一列建索引的依据
1. 列的数据发散
2. 列数据不变（是不是一点不能变）
3. 数据长度不能太长
4. 一个表不能建太多的索引
5. 联合查询的可以建联合索引

### sql的执行流程
1. mysql连接器做权限认证，用户名密码是否正确，该用户是否有该select语句的权限
2. 查询缓存
3. 分析器，词法分析和语法分析，词法分析就是提取关键字，如select，表名，字段名，语法分析就是sql语法是否正确
4. 优化器，以它认为的最优的执行方案去执行，比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。
5. 执行器，执行之前再校验一遍权限，调用innodb引擎的接口来执行sql

6. 如果是update的话，写undo log,用于提交失败后回滚
7. 去B+树种根据索引找到这一行数据，如果索引不是主键索引，并且查询的是不止索引字段，还要再回表一次，去主键索引的B+树查找，找到需要的那一行数据，返回
8. 判断数据页是否在内存中，是否是唯一性索引，来将update操作更改内存中的数据页或者更改磁盘文件，或者将操作缓存到changebuffer
9. 写redolog 将redo log设置为prepare状态。
10. 写binlog,redo log改成commit状态，提交事务

### mysql慢sql问题排查
1.偶尔很慢
   1.1 数据库在刷新脏页，update更新数据是先写redo log日志，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。如果redolog日志满了，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，就会导致我们平时正常的SQL语句突然执行的很慢
   1.2 拿不到锁，如果要判断是否真的在等待锁，我们可以用 show processlist这个命令来查看当前的状态
   
2. 一直很慢
    2.1 where字段没有索引
    2.2 没用上索引，例如where c-1=100,where pow(c,2) = 1000,
    2.3 数据库选错索引，例如，select * from t where 100 < c and c < 100000，走索引的话需要走两次，一次c索引，一次主键索引，可能不如扫描，只要扫描一次。所以mysql怎么判断走不走索引，采样部分数据，如果数据发散厉害，范围大，就走索引，如果发散范围不大，就不走索引，但是采样可能出现错误，刚好采样到范围小的数据，就导致选错索引。解决办法：强制使用索引，select * from t force index(a) where c < 100 and c < 100000;  采样错误也会导致where多个字段，有索引的话，选错索引
    
登录到MySQL中进行查看是否有事物未提交，是否有发生锁等待。
select * from information_schema.INNODB_TRX
select * from information_schema.PROCESSLIST #查看当前的SQL执行情况。主要观察是否有 Waiting for table metadata lock 或者表锁、全局读锁等SQL。

看看是否有大量临时表


### undo log是什么？
undo log主要是保证事务的原子性，事务执行失败就回滚，用于在事务执行失败后，对数据回滚。undo log是逻辑日志，记录的是SQL。（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。）

### change buffer是什么
（就是将更新数据页的操作缓存下来）
在更新数据时，如果数据行所在的数据页在内存中，直接更新内存中的数据页。
如果不在内存中，为了减少磁盘IO的次数，innodb会将这些更新操作缓存在change buffer中，在下一次查询时需要访问这个数据页时，在执行change buffer中的操作对数据页进行更新。
适合写多读少的场景，因为这样即便立即写了，也不太可能会被访问到，延迟更新可以减少磁盘I/O，只有普通索引会用到，因为唯一性索引，在更新时就需要判断唯一性，所以没有必要。

### redo log 是什么？
redo log就是为了保证事务的持久性。因为change buffer是存在内存中的，万一机器重启，change buffer中的更改没有来得及更新到磁盘，就需要根据redo log来找回这些更新。
优点是减少磁盘I/O次数，即便发生故障也可以根据redo log来将数据恢复到最新状态。
缺点是会造成内存脏页，后台线程会自动对脏页刷盘，或者是淘汰数据页时刷盘，此时收到的查询请求需要等待，影响查询。

### InnoDB事务实现原理
acid
原子性：
靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。
undo log属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。
以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。

一致性：

隔离性:
(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性

持久性：
靠的是redo log。当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。

### innodb和myisam的区别
1. innodb不保存表的具体行数
为什么innodb不保存：
因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询
2. InnoDB 支持事务，MyISAM 不支持事务
3. InnoDB 支持外键，而 MyISAM 不支持
4. InnoDB 是聚集索引，MyISAM 是非聚集索引。
5. InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁



# 8 elasticsearch
### 8.1 Elasticsearch是什么？
- elasticsearch是一个分布式的实时搜索和分析引擎

- Elasticsearch 也是 Master-slave 架构，也实现了数据的分片和备份。
- es⽂档写操作是分⽚上⽽不是节点上，先写在主分⽚，主分⽚再同步给副分⽚，因为主分⽚可以分布在不同的节点上，所以当集群只有⼀个master节点的情况下，即使流量的增加它也不会成为瓶颈，就算它挂了，任何节点都有机会成为主节点。
### 8.2 为什么使用elasticsearch
1. 存储的是非文档型数据
2. Elasticsearch使用的倒排索引比关系型数据库的B-Tree索引快
3. 文本搜索
4. 相关度排名

### es的架构原理 todo
分布式，分片，主分片，副分片，segment，

### 8.3什么是倒排索引
即根据文章内容中的关键字建立索引，索引会指向包含这个关键字的文档

### 8.4默认分词器
standard分词器

### 8.4 es的query大量数据报错的问题
```
SearchQuery query = new NativeSearchQueryBuilder().withQuery(matchAllQuery())
                .withTypes(TYPE_INTELLIGENT_COLLECT_LABEL.getType())
                .withIndices(EsIndex.INDEX_INTELLIGENT_BASE.getName())
                .withPageable(new PageRequest(0, 200,
                        new Sort(Sort.Direction.DESC,"createAt"))).build();

        Iterable<EntityCollectLabel> iterable = elasticsearchTemplate.queryForList(query,EntityCollectLabel.class);
```
这个方法会报数据量过大的错误
```
Caused by: QueryPhaseExecutionException[Result window is too large, from + size must be less than or equal to: [500000] but was [510000]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.]
  at org.elasticsearch.search.internal.DefaultSearchContext.preProcess(DefaultSearchContext.java:212)
```
需要用游标的方式取数据sroll api
```
String scrollId = elasticsearchTemplate.scan(searchQuery,55000,false);
 Page<EntityTrackingDi> page = elasticsearchTemplate.scroll(
                    scrollId, 55000L , new SearchResultMapper() {
```
原因 ：
***ElasticSearch的默认深度翻页机制***
ES默认的分页机制一个不足的地方是，比如有5010条数据，当你仅想取第5000到5010条数据的时候，ES也会将前5000条数据加载到内存当中，所以ES为了避免用户的过大分页请求造成ES服务所在机器内存溢出，默认对深度分页的条数进行了限制，默认的最大条数是10000条，

> 其实mysql的limit 100000,100 也是将十万多条数据加载到内存，也有同样的问题，但是mysql可以通过sql语句优化 比如
Select * From table Where ID>=(
    Select ID From table order by ID limit 100000,1
) order by ID limit 100;
SELECT id,title,content FROM items WHERE id IN (SELECT id FROM items ORDER BY id limit 900000, 10);
先查询id，这样加载到内存的只有id数据，不是全部数据。

scroll搜索会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索

es 提供了 scroll 的方式进行分页读取。原理上是对某次查询生成一个游标 scroll_id ， 后续的查询只需要根据这个游标去取数据，直到结果集中返回的 hits 字段为空，就表示遍历结束。scroll_id 的生成可以理解为建立了一个临时的历史快照，在此之后的增删改查等操作不会影响到这个快照的结果。

# es深度分页解决方案  scrollid会变吗
<https://my.oschina.net/u/1787735/blog/3024051>

# es调优，慢查询怎么处理


# 锁
### 并发编程的三个概念
- 原子性
即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。
① y=1;   原子操作
② y=x;   非原子操作。两步，取x值再赋值给y
③ y++;   非原子操作，三步，相当于y=y+1
- 可见性
可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。
- 有序性
即程序执行的顺序按照代码的先后顺序执行。实际jvm执行时会指令重排序，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。（多线程就不一定了）

参考<https://www.jianshu.com/p/94ae2257c747>

### 锁有哪几种
##### synchronized 
synchronized是java中加锁的关键字，可以用来给对象和方法或者代码块加锁，当它锁定一个方法或者一个代码块的时候，同一时刻最多只有一个线程可以执行这段代码。另一个线程必须等待当前线程执行完这个代码块以后才能执行该代码块。
参考<https://juejin.cn/post/6844903670933356551>
- 修饰实例方法
```
public class AccountingSync implements Runnable{
    static int i=0;
    public synchronized void increase(){
        i++;
    }
    ...
}
```
锁的是这个类的实例对象

- 修饰静态方法
```
public static synchronized void increase(){
    i++;
}
```
- 修饰类
```
synchronized (Singleton.class) {  
}
```
锁对象是当前类的class对象

- 修饰代码块
```
public void run() {
        static AccountingSync instance=new AccountingSync();
        synchronized(instance){
            for(int j=0;j<1000000;j++){
                    i++;
              }
        }
    }
```
锁的是instance对象

ReentrantLock
使用案例<https://www.liaoxuefeng.com/wiki/1252599548343744/1306580960149538>

### ReentrantLock,synchronized,volatile区别
- ReentrantLock 
可保证原子性,可见性。加锁方式同步，阻塞式的同步。
ReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。
默认创建非公平锁。也可创建公平锁。
一个ReentrantLock对象可以同时绑定对个对象。
等待可中断
可重入


- synchronized 
可保证原子性，可见性。加锁方式同步，阻塞式的同步。
Synchronized是java语言的关键字，是原生语法层面的互斥，需要jvm实现。
非公平锁
只能绑定一个对象
不能等待可中断
可重入

- volatile
可以保证可见性和有序性,不能保证原子性。只能作用于变量
**可见性**
使用volatile关键字会强制将修改的值立即写入主存,也就是线程2在修改完stop的值之后会把stop的值立即写入内存；
使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效,由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。
**有序性**
volatile关键字能禁止指令重排序。在volatile变量之前的指令不能在volatile之后执行，在volatile之后的指令也不能在volatile之前执行

### volatile防止指令重排原理
编译后生成 内存屏障指令
内存屏障指令前面的代码不可以越过这个屏障，后面的代码也不能越过这个屏障

### ReentrantLock,synchronized,volatile 实现原理
#### synchronized 原理
参考<https://www.cnblogs.com/aspirant/p/11470858.html>
synchronized 修饰代码块时，即锁的是对象
synchronized 有两条指令monitorenter，monitorexit。 
monitorenter：线程执行monitorenter指令时尝试获取monitor的所有权，过程如下：
1. 如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者；
2。 如果线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1；
3. 如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取monitor的所有权；
monitorexit：执行monitorexit的线程必须是objectref所对应的monitor的所有者。指令执行时，monitor的进入数减1，如果减1后进入数为0，那线程退出monitor，不再是这个monitor的所有者。其他被这个monitor阻塞的线程可以尝试去获取这个 monitor 的所有权。

synchronized 修饰实例方法时
常量池中多了 ACC_SYNCHRONIZED 标示符
当方法调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象。

两个指令的执行是JVM通过调用操作系统的互斥原语mutex来实现，被阻塞的线程会被挂起、等待重新调度，会导致“用户态和内核态”两个态之间来回切换，对性能有较大影响。

对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充
如图<20210609_1_object.png>
Synchronized用的锁就是存在Java对象头里的

#### synchronized应用场景
com.kedacom.kapsdatacollection.scene.loudi.service.impl.LouDiCollectionServiceImpl#departmentDataCollection
> 现场项目因为token一小时有效，同时token接口请求一天有上限，所以同步获取token

### cas
参考<https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&mid=2651749434&idx=3&sn=5ffa63ad47fe166f2f1a9f604ed10091&chksm=bd12a5778a652c61509d9e718ab086ff27ad8768586ea9b38c3dcf9e017a8e49bcae3df9bcc8&scene=38#wechat_redirect>
CAS算法涉及到三个操作数：
需要读写的内存值 V。
进行比较的值 A。
要写入的新值 B。
当寄存器中的 A 和 内存中的值 V 相等 ，就把要写入的新值 B 存入内存中，覆盖v。
如果不相等，就将内存值 V 赋值给寄存器中的值 A。然后通过Java代码中的while循环再次调用cmpxchg指令进行重试，直到设置成功为止。

##### cas的缺点
1. 循环时间长开销大
2. 只能保证一个共享变量的原子操作
3. ABA问题

##### java中cas的应用
1. AtomicInteger
2. 自旋锁
3. 令牌桶限流器

### 自旋锁
参考同上cas的链接
概念：线程尝试获取锁失败后，如果非自旋锁的情况下CPU会切换状态，使当前线程休眠，切换其他线程执行。而自旋锁不放弃CPU时间片，通过自旋等待锁释放。自旋锁的实现原理同样也是CAS。
为什么有自旋锁：阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。
缺点:不能代替阻塞，自旋等待虽然避免了线程切换的开销，但它要占用处理器时间。如果锁被占用的时间很短，自旋等待的效果就会非常好。反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。所以，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBlockSpin来更改）没有成功获得锁，就应当挂起线程。

### 自适应自旋锁
自适应意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。

### 无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁
参考同上cas的链接
![](20210610_1_suo)
这四种锁是指锁的状态，专门针对synchronized的。synchronized是悲观锁，在操作同步资源之前需要给同步资源先加锁，这把锁就是存在Java对象头里的。
synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的线程同步。依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”。
目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。

##### 无锁
无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。

##### 偏向锁
偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。
当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。
偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。

目的是：为了在没有多线程竞争的情况下尽量减少不必要的轻量级锁执行路径。因为轻量级锁的加锁解锁操作是需要依赖多次CAS原子指令的，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗也必须小于节省下来的CAS原子指令的性能消耗）。

##### 轻量级锁
是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。
对象Mark Word的锁标志位设置为“00”
若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。

轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。

##### 重量级锁
升级为重量级锁时，锁标志的状态值变为“10”。此时Mark Word中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。

综上，偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。

### 公平锁 VS 非公平锁
公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。
优点：等待锁的线程不会饿死。
缺点：整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。

非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。
优点：可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。
缺点：处于等待队列中的线程可能会饿死，或者等很久才会获得锁。

###  可重入锁 VS 非可重入锁
可重入锁
概念：是一个线程获取到一个对象的锁，未释放锁的情况下可以再次获取该对象的锁来执行其他加锁方法。
优点：避免死锁

示例代码：
```
public class Widget{
    public synchronized void do1(){
        System.out.println("1");
        do2();
    }
    public synchronized void do2(){
        System.out.println("2");
    }
}

static Widget widget = new Widget();
widget.do1();
widget.do2();
```
当一个线程获取到widget对象的锁时,执行do1方法，因为synchronized是可重入的，所以能再执行do2方法。如果不可重入就出现死锁。

### 独享锁 VS 共享锁
独享锁也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。
共享锁是指该锁可被多个线程所持有。如果线程T对数据A加上共享锁后，则其他线程只能对A再加共享锁，不能加排它锁。获得共享锁的线程只能读数据，不能修改数据。

### java8对synchronized的优化
java6引进四种锁的状态，无锁，偏向锁，轻量级锁，重量级锁，（偏向锁和轻量级锁应该用的cas原理）
java8对cas进行优化，同时大量线程cas，会导致都不能成功，使用分段cas的思路，内部维护一个cell数组，分别对数组的一个位置操作，最后将结果累加。比如cell大小为10，100个线程，则10个线程对cell[0]自增，10个线程对cell[1]自增。。。 

### JUC包有哪些类
java.util.concurrent 即java的并发包
1. ReentrantLock
2. AtomicInteger
3. CountDownLatch
4. ConcurrentHashmap
5. ThreadPoolExecutor
6. .....

### aqs解释
参考<https://zhuanlan.zhihu.com/p/86072774>
AQS就是一个并发包的基础组件，用来实现各种锁，各种同步组件的。
它包含了state变量、加锁线程、等待队列等并发中的核心组件。
ReentrantLock底层是基于AQS来实现的。
AbstractQueuedSynchronizer，抽象队列同步器
![](20210611_1_aqs)
![](20210611_2_aqs)
加锁时，线程1用cas方式将state变量置为1，成功则是加锁成功，加锁线程记录当前获取锁的是线程1。线程1使用重入锁会将state变量值再加1，线程2获取锁会先cas尝试将state由0变为1，失败则判断当前获取锁的线程是不是自己，不是则线程2加入等待队列。



# 实现多线程的方式
1. 继承Thread类，重写run方法 （无返回值）

2. 实现Runnable接口，重写run方法，实现Runnable接口的实现类的实例对象作为Thread构造函数的target （无返回值）

3. 通过Callable和FutureTask创建线程 （有返回值）

4. 通过线程池创建线程 （有返回值）

详细参考<https://zhuanlan.zhihu.com/p/47401636>


# 线程池

### 使用说明
参考<线程池的使用>

### 线程池的处理流程
1. 提交任务后，线程池先判断线程数是否达到了核心线程数（corePoolSize）。如果未达到线程数，则创建核心线程处理任务；否则，就执行下一步；
2. 接着线程池判断任务队列是否满了。如果没满，则将任务添加到任务队列中；否则，执行下一步；
3. 接着因为任务队列满了，线程池就判断线程数是否达到了最大线程数。如果未达到，则创建非核心线程处理任务；
4. 否则，就执行饱和策略，默认会抛出RejectedExecutionException异常。

### 线程池参数设计原则
1. corePoolSize = 80%情况下每秒任务数 * 每个任务执行时间 = 200 * 0.1 = 20
2. queueCapacity = (coreSizePool/taskcost) * responsetime=20/0.1 * 1 = 200;    // responsetime:系统允许容忍的最大响应时间，假设为1秒，即一个任务进来需要在1秒内被处理响应
3. maxPoolSize = (max(tasks)- queueCapacity)/ * taskcost = (800-200)/ * 0.1 = 60
4. keepAliveTime 用默认值 60s
5. 丢弃策略 用默认抛出异常

# 反射
### 正射
```
Apple apple = new Apple(); //直接初始化，「正射」
apple.setPrice(4);
```
### 反射
```
Class clz = Class.forName("com.chenshuyi.reflect.Apple");
Method method = clz.getMethod("setPrice", int.class);
Constructor constructor = clz.getConstructor();
Object object = constructor.newInstance();
method.invoke(object, 4);
```
上面两段代码的执行结果，其实是完全一样的。 
参考<https://www.cnblogs.com/chanshuyi/p/head_first_of_reflection.html>

// todo 反射在spring中的应用 

# jvm // todo
<>


# mq消息队列

### 优缺点
优点：解耦，异步，消峰
缺点：系统复杂性增加，系统的可用性降低

### 目前主流mq类型
RocketMQ 
abbitmq
Kafka

### rabbitmq kafka rocketmq区别
1. kafka：吞吐量高，消息失败不支持重试,轻量级的，高效的但是不保证安全的。一开始的目的就是用于日志收集和传输，适合产生大量数据的互联网服务的数据收集业务。
2. rabbitmq:低延迟，对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。有消息确认机制，和持久化功能。
开源的，社区比较稳定的支持，活跃度也高，社区文档资料完善些
3. RocketMQ 阿里开源的消息中间件，它是纯Java开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。
// todo rabbitmq和RocketMQ的区别

选型：
所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。
如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。

### rabbitmq原理
消息发送消费流程：
1. producer发送消息到mq的exchange组件
2. exchange组件根据routing key将message分发到对应的queue1中
3. queue1将消息发送到consumer
4. consume发送ack确认消息到queue1
5. queue1收到ack，删除队列中的message
（如果queue1没收到ack，则会一直发送message，即消息丢失超时重发）
参考<https://zhuanlan.zhihu.com/p/281912931> 

### rabbitmq的exchange路由规则
1. Fanout    （全局转发）
会将消息发送到所有和它进行绑定的队列上。
2. Direct   （根据routing key精确匹配）
通过消息上的路由键直接对消息进行分发。
3. Topic  （模糊匹配）
会将routing key和binding key进行通配符匹配。

### rabbitmq消息丢失处理策略：
消息丢失分为三种，1.发送方消息发送失败，2.exchange丢失消息，3.接收方未接受到消息
处理策略：
1.发送方开启confirm模式，每个消息会带一个唯一id，exchange收到消息会返回带id的确认消息。
2.exchange持久化消息（可以持久化后再返回确认消息）
3.接收方接受消息后返回ack确认，自动模式（默认，一接收到就返回ack，可能没来得及处理，或者选择手动ack，处理完消息才返回ack）
exchange返回ack后，发送方会有个回调函数就行处理，知道是成功还是失败，失败的话重发消息（消息哪来的，可以存进内存，比如用hashmap），如果ack丢失，或者就是没接收到，发送方设置超时时间，超时则重发消息。

### 假如是ack在网络中丢失，发送方或者exchange重新发送消息，接收方重复消费怎么办？
在消息生产时，MQ内部针对每条生产者发送的消息生成一个inner-msg-id，作为去重和幂等的依据（消息投递失败并重传），避免重复的消息进入队列；
在消息消费时，要求消息体中必须要有一个bizId（对于同一业务全局唯一，如支付ID、订单ID、帖子ID等）作为去重和幂等的依据，避免同一条消息被重复消费。

### mq消息的顺序性
1.通过某种算法，将需要保持先后顺序的消息放到同⼀个消息队列中(kafka中就是partition,rabbitMq中就是queue)。然后只⽤⼀个消费者去消费该队列。
2.可以在消息体内添加全局有序标识来实现。


# Spring中的设计模式
// todo 手写几种设计模式

1. 单例模式（bean ioc）
一般用这种就行了，如果要懒加载（即用到这个类变量再实例化，详见下面链接的第4和第5中实现方式）
```
public class Singleton {  
    private static Singleton instance = new Singleton();  
    private Singleton (){}  
    public static Singleton getInstance() {  
    return instance;  
    }  
}
```
参考<https://www.runoob.com/design-pattern/singleton-pattern.html>
2. 工厂模式（bean ioc）
实现参考<https://www.runoob.com/design-pattern/factory-pattern.html>
3. 代理模式（aop）
实现参考<https://www.runoob.com/design-pattern/proxy-pattern.html>
4. 观察者模式（applicationListener）
实现参考<https://www.runoob.com/design-pattern/observer-pattern.html>

# spring 启动过程
// todo
<https://search.bilibili.com/all?keyword=spring%E5%90%AF%E5%8A%A8&from_source=web_search>
<spring_start.png>

# 集合的原理
1. ArrayList
2. HashMap   (数组+链表+红黑树 线程不安全)
3. ConcurrentHashMap（线程安全）
3. HashTable   （线程安全 不为bull）
3. LinkedHashMap  （插入有序的hashmap）
4. TreeMap   （按键排序,底层是红黑树结构）

#### ArrayList
底层是数组。允许空值和重复值。默认初始化为空数组，第一次add后初始化为10。当空间用完，其会按照原数组空间的 1.5 倍进行扩容。即10->15->22 。
##### HashMap
参考<https://tech.meituan.com/2016/06/24/java-hashmap.html>
底层是数组+链表+红黑树。初始化后，第一次put会生成大小为16的数组。负载因子默认为0.75。put时，计算hashcode，在数组中的位置，key相同则替换value，不相同则在后面生成链表放入值。链表长度大于8则链表转换成红黑树，小于6时再转回链表。最后检测已存储数量是否为数组的0.75阈值，超过则resize扩容，扩容成原来的两倍
*为什么初始化是2的幂次方，扩容后也是2的幂次方*
// todo
*链表为啥从一开始的尾插法改为了头插法*
// todo
*为什么转换负载因子是0.75*
答：// todo 经验所致，这样存取效率最高
*为什么转换成红黑树阈值是8*
答：// todo 转换红黑树费时，太小转换不划算，太多转换，则链表存取费时，8大概是红黑树4层
*为什么红黑树转链表阈值是6而不是8*
答：因为避免插入删除时频繁转换
#### ConcurrentHashMap
jdk1.7采用Segment分段锁。1.8利用CAS+Synchronized来保证并发更新的安全，底层采用数组+链表+红黑树的存储结构。

1. 线程安全的，1.7之前是分段锁，1.8是cas+synchronized
put操作分两种情况，
1. 当前hash表对应当前key的index上没有元素时
cas的方式创建node放入数组
2.当前hash表对应当前key的index上已经存在元素时(hash碰撞)
synchronized对当前位置的节点头 加锁
> https://juejin.im/post/6844903813892014087
 // todo
#### HashTable
// todo
#### LinkedHashMap
// todo
#### TreeMap
// todo

# spring事务
参考：<https://zhuanlan.zhihu.com/p/114461128>

### 事务作用
可以让一系列操作变成一个原子操作，要么一起成功，要么一起失败，用于转账等类似场景

### 编程式事务
```
try {
    //TODO something
     transactionManager.commit(status);
} catch (Exception e) {
    transactionManager.rollback(status);
    throw new InvoiceApplyException("异常失败");
}
```
参考<https://cloud.tencent.com/developer/article/1697221>

### 声明式事务
```
@Transactional(rollbackFor = Exception.class)
    @GetMapping("/test")
    public String test() {
        int insert = cityInfoDictMapper.insert(cityInfoDict);
    }
```

### 基于@Transactional注解
@Transactional 可以作用在接口、类、类方法。
- 作用于类：当把@Transactional 注解放在类上时，表示所有该类的public方法都配置相同的事务属性信息。
- 作用于方法：当类配置了@Transactional，方法也配置了@Transactional，方法的事务会覆盖类的事务配置信息。
- 作用于接口：不推荐这种使用方法，因为一旦标注在Interface上并且配置了Spring AOP 使用CGLib动态代理，将会导致@Transactional注解失效
另外， @Transactional 注解应该只被应用到 public 方法上，这是由 Spring AOP 的本质决定的。如果你在 protected、private 或者默认可见性的方法上使用 @Transactional 注解，这将被忽略，也不会抛出任何异常。

### 传播级别
默认值为 Propagation.REQUIRED

- @Transactional(propagation=Propagation.REQUIRED) ：如果当前存在事务，则加入该事务，如果当前不存在事务，则创建一个新的事务。( 也就是说如果A方法和B方法都添加了注解，在默认传播模式下，A方法内部调用B方法，会把两个方法的事务合并为一个事务 ）
- @Transactional(propagation=Propagation.NOT_SUPPORTED) ：以非事务的方式运行，如果当前存在事务，暂停当前的事务。
- @Transactional(propagation=Propagation.REQUIRES_NEW) ：不管是否存在事务,都创建一个新的事务,原来的挂起,新的执行完毕,继续执行老的事务
- @Transactional(propagation=Propagation.MANDATORY) ：如果当前存在事务，则加入该事务；如果当前不存在事务，则抛出异常。
- @Transactional(propagation=Propagation.NEVER) ：以非事务的方式运行，如果当前存在事务，则抛出异常。
- @Transactional(propagation=Propagation.SUPPORTS) ：如果当前存在事务，则加入该事务；如果当前不存在事务，则以非事务的方式继续运行。
- @Transactional(propagation=Propagation.NESTED) ：和 Propagation.REQUIRED 效果一样。

### 隔离级别
默认是使用数据库的隔离级别

@Transactional(isolation = Isolation.READ_UNCOMMITTED)：读取未提交数据(会出现脏读, 不可重复读) 基本不使用
@Transactional(isolation = Isolation.READ_COMMITTED)：读取已提交数据(会出现不可重复读和幻读)
@Transactional(isolation = Isolation.REPEATABLE_READ)：可重复读(会出现幻读)
@Transactional(isolation = Isolation.SERIALIZABLE)：串行化

### spring事务失效的情况
参考<https://zhuanlan.zhihu.com/p/114461128>
1. 方法自调用
a方法没@transactional，b方法有@transactional，a调用b，代理类调用a，这时候不会触发b方法的事务。
(其实这还是由于使用Spring AOP代理造成的，因为只有当事务方法被当前类以外的代码调用时，才会由Spring生成的代理对象来管理。)
解决方法：
 - a上加@transactional，
 - 类上加@transactional
 - a中这样调用b ((A)AopContext.currentProxy).b()
 - 把b方法抽到另一个类中
2. 方法不是public
因为cglib或jdk代理的方法会调用computeTransactionAttribute方法，这个方法会检查目标方法是否是public，不是 public则不会获取@Transactional 的属性配置信息。protected、private 修饰的方法上使用 @Transactional 注解，虽然事务无效，但不会有任何报错
3. rollbackFor 设置错误，默认是RuntimeException，其他异常不会回滚
4. myisam引擎不支持事务
5. 事务方法里使用了try-catch
6. propagation设置错误 supports not_supported never

### spring的事务隔离级别和mysql的事务隔离级别不一致会这么样？
以 Spring 事务为准


# 接口高并发保护系统的方案
1.缓存（redis）    （例如将高频同时不需要实时更新的数据放在redis中）
2.限流            （单位时间内超出阈值的请求直接返回请等待或失败）
3.降级            （高峰期将非关键服务关闭，例如双11零点将其他的例如修改购物车地址，退货等服务关闭。用于解决资源不足的情况）

### 限流
1.信号量计数器方法
```
private final Semaphore permit = new Semaphore(10, true);
@PostMapping("/test")
    public String test(){
        try {
            permit.acquire();       // 如果已经有10个任务在处理，第11个请求会阻塞在这里
            log.info("处理请求===============>");
            Thread.sleep(2000);
        }catch (Exception e){
            log.error("error");
        }finally {
            permit.release();
        }
        return "success";
    }


```
2. 令牌桶算法
我们以一个恒定的速率向一个桶内放令牌，每次请求来的时候去桶里拿令牌，如果拿到了就继续后面的操作，如果没有拿到则等待。
```
public static void main(String[] args) {
        String start = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date());
        RateLimiter limiter = RateLimiter.create(1.0); // 这里的1表示每秒允许处理的量为1个
        for (int i = 1; i <= 10; i++) {
            double waitTime = limiter.acquire(i);// 请求RateLimiter, 超过permits会被阻塞
            System.out.println("cutTime=" + System.currentTimeMillis() + " call execute:" + i + " waitTime:" + waitTime);
        }
        String end = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date());
        System.out.println("start time:" + start);
        System.out.println("end time:" + end);
    }
```
RateLimiter limiter = RateLimiter.create(1.0) 创建一个限流器，每秒生成1个令牌；

limiter.acquire(i) 以阻塞的方式获取令牌，随着i的增加，需要的令牌数增多，则需要等待的时间也增加。

相比计数方法的优点：可以方便的改变速度

> **如果是秒杀，库存问题**
则用redis存库存，redis库存减到0，不再处理请求


# 助力时网络故障，同一个人对同一个团助力两次
线程并发问题，两个线程同时查询助力表，发现用户a未对团b助力过，然后开始助力，这就等于a给b助力了两次，规则是一个人只能助力一次
解决办法：
1.对（a.id b.id）加唯一性索引
2.事务里代码逻辑：查询-插入-查询-校正，两个线程并发，其中一个第二次查询发现insert了两次就校正，如果两个线程都校正了，则返回助力失败。
3.没有设置唯一索引的话用 INSERT INTO table(field1, field2, fieldn) SELECT 'field1', 'field2', 'fieldn' FROM DUAL WHERE NOT EXISTS(SELECT field FROM table WHERE field = ?)
4.redis加分布式锁，以a的id和b的id为key加锁，锁的粒度为那一行数据。

# 更新开团表商品剩余价值时，并发执行问题
1. 可以用redis加分布式锁
2. 可以cas原理，比较version是否一样
3. 可以select  。。。。for update 加行锁



# 缓解数据库压力的办法
1. 缓存
2. 分库分表
3. 读写分离
4. 合理增加索引

# 什么是redis
2.Reids的特点
Redis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。

Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能，比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。

Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。

# redis应用场景
分布式缓存  
分布式锁    数据库并发插入，秒杀系统
计数器     网站浏览量，视频播放次数
轻量的消息队列
排行榜 zset
最新列表  list
存储session   博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。

# redis数据类型
Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。

String:
会采用预分配冗余空间的方式来减少内存的频繁分配，实际分配的空间 capacity 一般要高于实际字符串长度 len 
当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。

list 采用的存储结构是双向链表
在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成 quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next。所以 Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。

Redis hash 是一个键值(key=>value)对集合。
hash 与 Java 中的 HashMap 差不多，实现上采用二维结构，第一维是数组，第二维是链表。hash 的 key 与 value 都存储在链表中，而数组中存储的则是各个链表的表头。在检索时，首先计算 key 的 hashcode，然后通过 hashcode 定位到链表的表头，再遍历链表得到 value 值。
初始化大小为4

set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。

参考<https://blog.51cto.com/u_14032861/2998549>

# redis hash
在Redis中，键值对（Key-Value Pair）存储方式是由字典（Dict）保存的，而字典底层是通过哈希表来实现的。通过哈希表中的节点保存字典中的键值对。我们知道当HashMap中由于Hash冲突（负载因子）超过某个阈值时，出于链表性能的考虑，会进行Resize的操作。Redis也一样。

在redis的具体实现中，使用了一种叫做渐进式哈希(rehashing)的机制来提高字典的缩放效率，避免 rehash 对服务器性能造成影响，渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。

扩容是扩容为原来的2倍，如果原来不是2的n次方
则是大于等于size的第一个2的n次方

size初始化为4
hash大小size同时也是扩容的阈值

dictht ht[2]：在字典内部，维护了两张哈希表。 一般情况下， 字典只使用 ht[0] 哈希表， ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用
```
typedef struct dict {
    dictType *type;
    void *privdata;
    dictht ht[2];
    long rehashidx; /* rehashing not in progress if rehashidx == -1 */
    unsigned long iterators; /* number of iterators currently running */
} dict;

/* This is our hash table structure. Every dictionary has two of this as we
 * implement incremental rehashing, for the old to the new table. */
typedef struct dictht {
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigned long used;
} dictht;

typedef struct dictEntry {
    void *key;                //键
    union {
        void *val;            //值
        uint64_t u64;
        int64_t s64;
        double d;
    } v;
    struct dictEntry *next; //指向下一个节点，形成链表
} dictEntry;
```
扩容条件 
```
/*
     * 如果哈希表ht[0]中保存的key个数与哈希表大小的比例已经达到1:1，即保存的节点数已经大于哈希表大小
     * 且redis服务当前允许执行rehash，或者保存的节点数与哈希表大小的比例超过了安全阈值（默认值为5）
     * 则将哈希表大小扩容为原来的两倍
     */
 if (d->ht[0].used >= d->ht[0].size &&
        (dict_can_resize ||
         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio))
    {
        return dictExpand(d, d->ht[0].used*2);
    }
```

缩容
当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。
缩容后的大小为第一个大于等于当前key数量的2的n次方。最小容量为4。
同样从dictResize函数中可以看到，如果当前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，则不进行缩容

## 渐进式hash小结
在redis中，扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面， 但是， 这个 rehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。

以下是哈希表渐进式 rehash 的详细步骤：

（1）为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。

（2）在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。

（3）在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。

（4）随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。

渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。

# rehash的其他细节和缺点
渐进式 rehash 执行期间的哈希表操作
因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。

另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。

#渐进式rehash带来的问题
渐进式rehash避免了redis阻塞，可以说非常完美，但是由于在rehash时，需要分配一个新的hash表，在rehash期间，同时有两个hash表在使用，会使得redis内存使用量瞬间突增，在Redis 满容状态下由于Rehash会导致大量Key驱逐。


# redis主从同步

# 单线程的redis为什么这么快
(一)纯内存操作
(二)单线程操作，避免了频繁的上下文切换
(三)采用了非阻塞I/O多路复用机制 （多路-指的是多个socket连接，复用-指的是复用一个线程，采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求。非阻塞指不会因为一个连接阻塞其他连接的请求）
(redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。)

>**多路复用原理**
在多路复用 I/O 模型中，会有一个线程不断去轮询多个 socket 的状态，只有当 socket 真正有读写事件时，才真正调用实际的 I/O 读写操作。因为在多路复用 I/O 模型中，只需要使用一个线程就可以管理多个 socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有在真正有 socket 读写事件进行时，才会使用 I/O 资源，所以它大大减少了资源占用（如 CPU）

# redis为什么是单线程的
因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。

// todo 新版本redis是多线程了

# redis的四个问题
(一)缓存和数据库双写一致性问题
    正常流程：先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。
(二)缓存雪崩问题
(三)缓存穿透问题
(四)缓存的并发竞争问题

# redis删除策略
redis采用的是定期删除+惰性删除策略。
定期删除，redis默认每隔100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。
于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。
采用定期删除+惰性删除就没其他问题了么?
不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制（详见下文）。

# redis内存淘汰策略
noeviction(默认策略)：对于写请求不再提供服务，直接返回错误（DEL请求和部分特殊请求除外）
volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
allkeys-lru：（推荐）从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰
allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰
no-enviction（驱逐）：禁止驱逐数据

# 缓存穿透
缓存穿透是指查询一个一定不存在的数据（数据库也不存在），由于缓存的写入是在缓存中查不到，数据库中查到后被动写入，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。
***解决方案：***
(一)利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试
(二)采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做缓存预热(项目启动前，先加载缓存)操作。
(三)布隆过滤器

有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。
布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。

# 缓存雪崩
缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。
***解决方案：***
这里分享一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。

# 缓存击穿
缓存击穿跟缓存雪崩有点像，但是又有一点不一样，缓存雪崩是因为大面积的缓存失效，打崩了DB，而缓存击穿不同的是缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个完好无损的桶上凿开了一个洞。
***解决方案：***
1. 将热点数据设置为永远不过期
2. 或者基于 redis or zookeeper 实现互斥锁，缓存失效后，等待失效后的第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。

# 缓存并发竞争问题
问题描述： 同时有多个客户端去set一个key。

解决办法：
(1)乐观锁，使用对修改顺序没有要求的场景
(1)如果对这个key操作，不要求顺序
这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。
(2)如果对这个key操作，要求顺序
假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.
期望按照key1的value值按照 valueA-->valueB-->valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下

系统A key 1 {valueA  3:00}
系统B key 1 {valueB  3:05}
系统C key 1 {valueC  3:10}
那么，假设这会系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。

其他方法，比如利用队列，将set方法变成串行访问也可以。总之，灵活变通。

# redis分布式锁是怎么实现的
使用setnx方法来争抢锁，抢到之后，再用expire方法给锁加一个过期时间防止锁忘记了释放。 
```
if (setnx(key, 1) == 1){
    expire(key, 30)
    try {
        //TODO 业务逻辑
    } finally {
        del(key)
    }
}
```

如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？
1.可以同时把setnx和expire合成一条指令来用    set key value nx ex 5
2.使用 lua 脚本

参考<https://juejin.cn/post/6855129006619459592>

# redis zset原理


## 超时解锁导致并发
如果线程 A 成功获取锁并设置过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁，线程 A 和线程 B 并发执行。

解决办法：
1.将过期时间设置足够长，确保代码逻辑在锁释放之前能够执行完成。
2.为获取锁的线程增加守护线程，为将要过期但未释放的锁增加有效时间。

## 锁误解除
如果线程 A 成功获取到了锁，并且设置了过期时间 30 秒，但线程 A 执行时间超过了 30 秒，锁过期自动释放，此时线程 B 获取到了锁；随后 A 执行完成，线程 A 使用 DEL 命令来释放锁，但此时线程 B 加的锁还没有执行完成，线程 A 实际释放的线程 B 加的锁。

解决办法：通过在 value 中设置当前线程加锁的标识，在删除之前验证 key 对应的 value 判断锁是否是当前线程持有。可生成一个 UUID 标识当前线程，使用 lua 脚本做验证标识和解锁操作。


# redis主从复制
为了保证 Redis 的可用性，一般采用主从方式部署。主从数据同步有异步和同步两种方式，Redis 将指令记录在本地内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流来达到和主节点一致的状态，一边向主节点反馈同步情况。

# redis缓存数据库双写一致性
// todo
有四种方案
1.更新数据库，更新缓存
2.更新缓存，更新数据库
3.删除缓存，更新数据库`
4.更新数据库，删除缓存

1. 1的问题在于，如果缓存不是直接拿的数据库值，而是经过一个复杂的计算，则这样每次都重新计算下，但是又不怎么读这个数，太费资源。
2. 2的问题和1的问题一样
3. 3的问题在于，并发请求时，请求a删除redis，更新数据库，但没提交，这时b查询，redis没有数据，数据库因为还没提交也没数据，解决方法：事务包括删redis，更新数据库
4. 正确的做法，但是还是有问题：更新数据库后，删除缓存失败。解决：删除缓存失败后将key值扔进消息队列，再从消息队列中取key来删除，问题：和业务代码耦合太深，解决:从binlog日志中读取对mysql的操作，然后将key扔进mq中再删除


# redis需要掌握的
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_4.png)

# 线程同步的方法
1. synchronized
2. reentrantlock
3. linkedBlockingQueue
4. AtomicInteger


# epoll原理 //todo

# java接口架构图
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_1.png)


# fork join  // todo


# mybatis原理 // todo 
mybatis访问数据库有两种方式
1. mybatis api
创建一个和数据库打交道的SqlSession对象，然后根据Statement Id 和参数来操作数据库
2. mapper接口
通过动态代理机制生成一个Mapper 实例，我们使用Mapper接口的某一个方法时，MyBatis会根据这个方法的方法名和参数类型，确定Statement Id，底层还是通过SqlSession.select("statementId",parameterObject);或者SqlSession.update("statementId",parameterObject); 等等来实现对数据库的操作

原理
MyBatis启动时，解析mybatis的配置文件，并且从指定路径下解析mapper.xml配置文件 
把每条sql语句映射成MappedStatement
然后把MappedStatement存放到Configuration的一个mappedStatements属性中（mappedStatements是一个HashMap），key为namespace + id，value为MappedStatement
当要执行sql语句的时候，从mappedStatements这个map中通过id找到MappedStatement
获取MappedStatement对应sql语句、查询参数
查看一级缓存中有没有数据，有则直接返回
缓存没有数据，则查询数据库 
通过调用原生的jdbc方法，执行sql语句，获取到结果，删除旧缓存
把结果放到一级缓存，返回结果

# mybatis架构图
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_2.png)

<https://blog.csdn.net/luanlouis/article/details/40422941>

<https://www.jianshu.com/p/ec40a82cae28>

// todo 找下mybatis的面试题


# 接口反应时间长的问题排查（接口慢）
1. 资源瓶颈 cpu是否100% （加机器）
2. 线程池配置的不合理（线程数配置的太少导致的请求积压）
3. 是否多表关联  把join逻辑放到业务代码里解决
2. 缓存没加 （加缓存）
3. 查看gc是否频繁GC，GC时间过长
4. 依赖第三方接口 （是不是第三方接口反应慢，能不能异步调用）
5. sql查询慢 （大量临时表，索引有没有加，explain有没有用到索引，是否需要强制需要用索引，数据量太大，要扩容）

# 聚簇索引和非聚簇索引区别
1. 聚簇索引的主键索引叶节点存放整行数据，非聚簇索引叶节点存放数据地址
2. 聚簇索引的数据的物理存放顺序与索引顺序是一致的
3. 聚簇索引的辅助索引是指向主键索引，而非聚簇索引的辅助索引（也即二级索引）是指向数据的物理地址。即每个索引相对独立，查询用到索引时，索引指向数据的位置。

> innodb使用聚簇索引，myisam使用非聚簇索引


# 聚簇索引的好处
1. InnoDB在移动行时无须更新辅助索引中的这个"指针"
2. 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问，不必访问磁盘。
3. 聚簇索引适合用在排序的场合，非聚簇索引不适合
4. 取出一定范围数据的时候，使用用聚簇索引
5. 非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转。聚簇索引则只需一次I/O。

# 聚簇索引劣势
1. 二级索引访问需要两次索引查找
2. 插入速度严重依赖于插入顺序
3. 更新主键的代价很高，因为将会导致被更新的行移动
4. 采用聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多（https://juejin.im/post/6844903845554814983）



# spring bean创建过程  // todo


# spring如何解决循环依赖
### 产生循环依赖的代码
属性注入：
```
@Service
public class CircularServiceA {
    @Autowired
    private CircularServiceB circularServiceB;
}
@Service
public class CircularServiceB {
    @Autowired
    private CircularServiceC circularServiceC;
}
@Service
public class CircularServiceC {

    @Autowired
    private CircularServiceA circularServiceA;

}
```


### 解决循环依赖
首先，需要明确的是spring对循环依赖的处理有三种情况：
①构造器的循环依赖：这种依赖spring是处理不了的，直接抛出BeanCurrentlylnCreationException异常。
②单例模式下的setter循环依赖：通过“三级缓存”处理循环依赖。（上面的例子就是单例模式下的基于setter注入的循环依赖）
③非单例循环依赖：无法处理。

2的情况原理：
spring三级缓存
我们初始化一个Bean时，先调用Bean的构造方法，这个对象就在内存中存在了（对象里面的依赖还没有被注入），然后把这个对象保存下来，当循环依赖产生时，直接拿到之前保存的对象，于是循环依赖就被终止了，依赖注入也就顺利完成了。

https://blog.csdn.net/lkforce/article/details/97183065
https://juejin.im/post/6844903806757502984

> setter注入：
  ```
  @Controller
  public class FooController {
    private FooService fooService;
    @Autowired
    public void setFooService(FooService fooService) {
        this.fooService = fooService;
    }
  }
  ```
  构造器注入：
  ```
  @Controller
  public class FooController {
    
    private final FooService fooService;
    
    @Autowired
    public FooController(FooService fooService) {
        this.fooService = fooService;
    }
  }
  ```

# java怎么解决循环依赖
1. 重新设计类结构
2. 注解 @Lazy
```
@Component
public class CircularDependencyA {
 
    private CircularDependencyB circB;
 
    @Autowired
    public CircularDependencyA(@Lazy CircularDependencyB circB) {
        this.circB = circB;
    }
}
```
3. 使用Setter/Field注入
```
@Autowired
public void setCircB(CircularDependencyB circB) {
    this.circB = circB;
}
```
4. 使用@PostConstruct
```
@Component
public class CircularDependencyA {
 
    @Autowired
    private CircularDependencyB circB;
 
    @PostConstruct
    public void init() {
        circB.setCircA(this);
    }
 
    public CircularDependencyB getCircB() {
        return circB;
    }
}
```
5. 实现ApplicationContextAware与InitializingBean

参考<https://blog.csdn.net/Revivedsun/article/details/84642316>


# sql：while col1 like '%key' 怎么用索引
// 创建一个函数索引
CREATE INDEX inde_1 ON table1(REVERSE(col1));
// 使用
SELECT * FROM table1 WHERE REVERSE(col1) LIKE REVERSE('%ABC');

# @autowired原理
根据类型自动装配
@Autowired   
@Qualifier("userServiceImpl")   
public IUserService userService; 
加上@Qualifier 可以根据名字就行装配

# explain除了看索引还看什么 
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_3.png)
<https://mengkang.net/1124.html>
<https://blog.51cto.com/lijianjun/1881208>

主要看哪些字段：
type：
一般来说，得保证查询至少达到range级别，最好能达到ref。
key:
列显示MySQL实际决定使用的键（索引）
key_len：
显示MySQL决定使用的键长度
Extra：
Using temporary 用临时表保存中间结果，常用于GROUP BY 和 ORDER BY操作中，一般看到它说明查询需要优化了，就算避免不了临时表的使用也要尽量避免硬盘临时表的使用。


# mybatis缓存
### 一级缓存
默认开启：一级缓存是本地（局部）缓存，不能被关闭，只能配置缓存范围：SESSION或STATEMENT。也就是说一级缓存不需要在配置文件去配置，默认开启。？
结论：不开启事务时，一级缓存不生效；开启了事务，一级缓存生效
原因：因为它每条语句执行结束以后，都会执行commit提交方法，而提交方法在每次都会清空本地缓存。而开启了事务的话，方法是在所有操作结束以后才会提交，因此就会支持一级缓存啦
### 二级缓存
默认情况下，mybatis打开了二级缓存，但它并未生效，因为二级缓存的作用域是namespace，所以还需要在Mapper.xml文件中配置一下才能使二级缓存生效

# g1垃圾收集器，强软弱虚引用，gc算法

# threadlocal
定义: 可以创建线程私有变量
作用:    1. 实现线程安全
        2. 实现单个线程单例以及单个线程上下文信息存储，比如交易id等
        3. 承载一些线程相关的数据，避免在方法中来回传递参数
原理
threadlocal.set()
就是往threadlocalMap里set东西
ThreadLocalMap是ThreadLocal的内部类
map中key是当前ThreadLocal对象
Thread为每个线程维护了ThreadLocalMap这么一个Map，而ThreadLocalMap的key是LocalThread对象本身，value则是要存储的对象
每个Thread维护着一个ThreadLocalMap的引用


# ThreadLocal内存泄漏
原因：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏

可是ThreadLocal并不会产生内存泄露，因为ThreadLocalMap在选择key的时候，并不是直接选择ThreadLocal实例，而是ThreadLocal实例的弱引用。

map里是弱引用，只要gc就会回收，不会发生内存泄漏




# cpu100%排查 ? 待完善
1. 定位高负载进程 pid，使用top命令
2. 定位具体的异常业务，使用 pwdx 命令根据 pid 找到业务进程路径，进而定位到负责人和项目
3. 定位异常线程及具体代码行
传统的方案一般是4步：

top oder by with P：1040 // 首先按进程负载排序找到  maxLoad(pid)
top -Hp 进程PID：1073    // 找到相关负载 线程PID
printf “0x%x\n”线程PID： 0x431  // 将线程PID转换为 16进制，为后面查找 jstack 日志做准备
jstack  进程PID | vim +/十六进制线程PID -        // 例如：jstack 1040|vim +/0x431 -


1. 定位高负载进程 pid，使用top命令
2. 显示线程列表 ps -mp pid -o THREAD,tid,time

# 淘宝首页性能优化
从技术的角度

# 内存泄漏怎么排查问题

# 使用反射 怎么实现一个rpc框架

# mybatis怎么写 like查询，即不同于普通的like查询


# 线程安全的hashmap
hashtable  set get方法上加synchronized
synchronizedMap    synchronized获取mutex锁
concurrentHashMap


# 线程池如何保证核心线程数量不变 ？待完善
线程池当未调用 shutdown 方法时，是通过队列的 take 方法阻塞核心线程（Worker）的 run 方法从而保证核心线程不被销毁的。

核心线程数未超过 corePoolSize，每添加新的任务（command），都会创建新的线程（Worker中创建），即使有空闲线程存在；
![](https://github.com/DavidSuperM/davidsuperm.github.io/blob/master/images/pic_20210520_6.png)

<https://blog.csdn.net/smile_from_2015/article/details/105259789>

# spring自动注入
@Autowired 根据类型注入， 
@Resource 默认根据名字注入，其次按照类型搜索
@Autowired @Qualifie("userService") 两个结合起来可以根据名字和类型注入

# todo
线程池怎么保证core线程数量不变
g1的原理，好处
分库分表是对 groupid这个字段进行hash，这有什么缺陷
synchronized无锁状态怎么实现的
es怎么提高查询效率
spring原理，动态代理原理，rpc框架动态代理原理
lru

# spring bean初始化流程

# spring ioc启动过程

# jvm gc算法 g1的特点

# jvm类加载机制 过程

# mysql事务执行过程

# bean工厂和工厂bean

# mysql分库分表 平滑扩容方案

# 接口响应时间长调优

# to整理
### volitile为什么不能保证线程安全
<https://www.cnblogs.com/laipimei/p/11857786.html>

## 数据库平滑扩容的方案
<https://zhuanlan.zhihu.com/p/37792971>
<https://cloud.tencent.com/developer/article/1420754>

